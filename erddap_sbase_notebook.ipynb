{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *******************************************************\n",
    "#          <<<   Python ERDDAP Interface  >>>           #\n",
    "#                                                       #\n",
    "#      Functions:                                       #\n",
    "#       (a) search by catagory, protocol, keyword       #\n",
    "#       (b) request / process data from search / url    #\n",
    "#       (c) examine/QC metadata and other information   #\n",
    "#       (d) create, modify ScienceBase record           #\n",
    "#                                                       #\n",
    "#       Simple prototyping and tasks, for the moment    #\n",
    "#                                                       #\n",
    "#       T. Wellman, BCB, Core Sciences, USGS            #\n",
    "#       W.I.P. << DRAFT >> components mostly etched out # \n",
    "#       Version 0.1 to 0.2   9/1/2016,  1/15/2017       #\n",
    "# *******************************************************\n",
    "\n",
    "# DRAFT NOTES (DIARY):\n",
    "\n",
    "# This program attempts to strike a balance between using an\n",
    "# ERDDAP server directly and working mainly within a Python\n",
    "# environment for processing files and information to ScienceBase.\n",
    "# The visual interface of ERDDAP is very intuitive and easy\n",
    "# to use, so replacing it is not the primary goal. The \n",
    "# basic idea here is to interact with ERDDAP and automate\n",
    "# some of the tedium involved in retrieving ERDDAP files, error \n",
    "# checking, and setting up or updating a ScienceBase item.\n",
    "\n",
    "# A data search can be performed fresh or by repeating past searches\n",
    "# using an \"advanced search url\" generated by ERDDAP. The input \n",
    "# information block (below) is presently used for processing options.\n",
    "# Pan or search \"Main Program\" to find the notebook cell below for inputs. \n",
    "# The search url and general file paths are included in the \"sb_json\" file.\n",
    "# A name dictionary holds custom information to populate Sciencebase records.\n",
    "# The name dictionary is created beforehand (search \"Name Dictionary\")\n",
    "# in this notebook. Both the name dictionary and sb_json file should be in the\n",
    "# specified working directory (tempdir). See the input section with paths \n",
    "# and such to specify in the \"Main Program\". Inputs and such are a bit \n",
    "# hardwired for now.\n",
    " \n",
    "# The \"url_flag\" shows whether (True, False) an existing url has been created. \n",
    "# The adv. url is in \"General information\" section of the \"sb_json\" file (below), \n",
    "# as the search url in json and html. It is easy to manually query an \n",
    "# external erddap to generate a url and piece together the \"General information\"  \n",
    "# section manually to bypass the internal query process. Or just indicate False\n",
    "# and use the internal functions to generate this information block automatically.\n",
    "#  --> url_flag = True or False\n",
    "    \n",
    "# Purge functions indicate whether to overwrite content used in all processing (proc) or sciencbase (sb)\n",
    "# This is useful in updating files, restarting runs after making code changes or connection failures.\n",
    "# Set overall purge (purge_proc = True) ONCE then flip to False if restart is needed. \n",
    "# purge_sb can also use a 'reset' to reset only the upload status to ScienceBase.\n",
    "#  -->  purge_proc = True or False\n",
    "#  -->  purge_sb = True, False, or 'reset'\n",
    "\n",
    "# Note currently uploading large files to ScienceBase can be an issue. The PYSB upload/replace functions\n",
    "# may need some tweaks. Offsite uploads at my home were problematic likely due to slow upload speeds.\n",
    "# There are options to upload large files to ScienceBase indicated on the website that have not been\n",
    "# explored here at present. \n",
    "\n",
    "# Adjust main program inputs first and save changes, then run next four notebook cells below, in order\n",
    "\n",
    "# This is a work in progress........ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import pysb\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "from os import path, makedirs, remove\n",
    "from shutil import copyfileobj\n",
    "from urllib import urlencode, quote \n",
    "from collections import OrderedDict\n",
    "from IPython.core.display import display\n",
    "from timeit import default_timer as timer\n",
    "from lxml import etree, objectify\n",
    "\n",
    "print('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple dictionary of ERDDAP file types and associated explanations\n",
    "\n",
    "fnames =['.asc','.csv','.csvp','.csv0','.das','.dds','.dods','.esriCsv','.fgdc',\n",
    "         '.geoJson','.graph','.help','.html','.htmlTable','.iso19115','.json',\n",
    "         '.mat','.nc','.ncHeader','.ncCF','.ncCFHeader','.ncCFMA','.ncCFMAHeader',\n",
    "         '.odvTxt','.subset','.tsv','.tsvp','.tsv0','.xhtml','.graph','.subset']\n",
    "\n",
    "describe = [' OPeNDAP-style comma-separated ASCII text',\n",
    "    'Comma-separated value file (ISO 8601)',\n",
    "    'Comma separated value file (name(units))',\n",
    "    'Comma separated value file (no names/units)',\n",
    "    'OPeNDAP Dataset Attribute Structure (DAS)',\n",
    "    'OPeNDAP Dataset Descriptor Structure (DDS)',\n",
    "    'OPeNDAP clients, download data DODS format',\n",
    "    'Comma separated value file ESRI ArcGIS 9.x',\n",
    "    'FGDC .xml metadata',\n",
    "    'GeoJSON .json file',\n",
    "    'View a Make A Graph web page',\n",
    "    'Description of tabledap',\n",
    "    'OPeNDAP-style HTML Data Access Form',\n",
    "    'html data web page',\n",
    "    'ISO 19115-2 .xml metadata',\n",
    "    'JSON file',\n",
    "    'Download a MATLAB binary file.',\n",
    "    ' NetCDF-3 binary file (COARDS/CF/ACDD)',\n",
    "    'Netcdf header (metadata)',\n",
    "    'NetCDF-3 CF DSG file',\n",
    "    'NetCDF header (metadata)',\n",
    "    'NetCDF-3 CF DSG file',\n",
    "    'Header (the metadata) for the .ncCFMA file',\n",
    "    'ODV Generic Spreadsheet File (.txt)',\n",
    "    'HTML form for subsetting data',\n",
    "    'Tab-separated ASCII text table ( ISO 8601 times)',\n",
    "    'Tab-separated ASCII text table ( line 1: name (units))',\n",
    "    'A .tsv file without column names or units',\n",
    "    'XHTML (XML) file with the data in a table',\n",
    "    'OPeNDAP raw data graphical processing',\n",
    "    'OPeNDAP raw data subsetting']\n",
    "\n",
    "file_dict = dict(zip(fnames, describe ))\n",
    "print('ERDDAP file dict loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# *******************************************************\n",
    "#    Definitions:  status = draft, work in progress \n",
    "# *******************************************************\n",
    "\n",
    "# write xml to python dictionary using recursion\n",
    "def xml_to_dict(xml_str):\n",
    "    def xml_to_dict_recursion(xml_object):\n",
    "        dict_object = xml_object.__dict__\n",
    "        if not dict_object:  # if empty dict returned\n",
    "            return xml_object\n",
    "        for key, value in dict_object.items():\n",
    "            dict_object[key] = xml_to_dict_recursion(value)\n",
    "        return dict_object\n",
    "    xml_obj = objectify.fromstring(xml_str)\n",
    "    return {xml_obj.tag: xml_to_dict_recursion(xml_obj)}\n",
    "\n",
    "# Adapt to Python v2+ ascii conversion issues (apparently fixed in python 3) \n",
    "def removeNonAscii(df_ascii): \n",
    "    df_ascii = df_ascii.apply(\n",
    "        lambda x: ''.join([i if 32 < ord(i) < 126 else \" \" for i in x]))\n",
    "    return df_ascii\n",
    "\n",
    "# Basic stats on requested ERDDAP data table (super simple for now)\n",
    "def get_stats(df_o):\n",
    "    agencies, counts = np.unique(df_o.loc[df_o[\"Institution\"] != (\"???\" or \"\")][\"Institution\"],return_counts=True)    \n",
    "    agencies = np.append(agencies, \"Unknown\")\n",
    "    counts = np.append(counts, df_o.shape[0] - np.sum(counts))\n",
    "    stats = [df_o[\"griddap\"].count(),\n",
    "             np.count_nonzero(df_o[\"tabledap\"]!=\"\"),\n",
    "             np.count_nonzero(df_o[\"griddap\"]!=\"\"),\n",
    "             len(agencies)-1 ]\n",
    "    print( \"\\n***** Query Results *****\\n\\nTotal number of datasets: %d\\nNumber of tables: %d\\nNumber of grids: %d\\\n",
    "        \\nNumber of agency groups providing data: %d\\n\" % tuple(stats))\n",
    "    header =  [\"** Agency Groups ** \", \"** Dataset count **\"]\n",
    "    a = max(len(str(max(agencies))),len(header[0]))\n",
    "    c = max(len(str(max(counts))),len(header[0]))\n",
    "    f  = '\\t{0:<%d}\\t{1:<%d}' % (a, c)\n",
    "    print(f.format(header[0], header[1]))\n",
    "    for p in zip(agencies, counts):\n",
    "        print(f.format(p[0],p[1]))\n",
    "    return\n",
    "\n",
    "# Show table\n",
    "def viewtable(df_view, cw, mr):\n",
    "    pd.set_option('display.max_colwidth', cw)\n",
    "    df_view.describe() \n",
    "    sys.stdout.write(\"\\nDisplay limited to a maximmum of %d datasets:\\n\" % mr)\n",
    "    df_view.columns.name = \"Data Index\"\n",
    "    display(df_view.ix[0:mr-1]) # assume 0 start index\n",
    "    return\n",
    "\n",
    "# bulk data download options: all data, all tables, or all grids  - refine later **\n",
    "def bulkoption(df):\n",
    "    bulkdict = {}\n",
    "    url = []\n",
    "    opts = [\"D\", \"T\", \"G\", \"C\"]\n",
    "    task = [\"all datasets\", \"all tables\", \"all grids\", \"by category\"]\n",
    "    resource = [ \"info\" , \"tabledap\", \"griddap\", \"categorize\"]\n",
    "    \n",
    "    for r in resource:\n",
    "        url.append(''.join(df.loc[df['Resource'] == r]['URL']))      \n",
    "    combo = zip(resource, task, url)\n",
    "    \n",
    "    # combine search info into dictionary\n",
    "    bulk_opts = zip(opts, combo)\n",
    "    for o, t in bulk_opts:\n",
    "        bulkdict[o] = t\n",
    "    return bulkdict\n",
    "\n",
    "# Generate url based on category and/or word/phrase search, \n",
    "# Note: doesn't include value constraints - incomplete, a bit clunky, refine later **\n",
    "def build_url(srch_word, protocol, cat_table, cindx, subcategory):\n",
    "    adv_url_html =  \"{}{}\".format(baseurl, \"/search/advanced.html?\")\n",
    "    prot_dict = {}\n",
    "    prot_dict = {\"A\" : \"(any)\", \"G\" : \"griddap\",\n",
    "                 \"W\" : \"WMS\",\"T\" : \"tabledap\"}\n",
    "    val_dict = OrderedDict()\n",
    "    val_dict = {\"maxLat\": \"\", \"minLon\": \"\", \"maxLon\": \"\",\n",
    "                \"minLat\": \"\", \"minTime\": \"\", \"maxTime\": \"\"}\n",
    "    udict = OrderedDict()\n",
    "    udict[\"searchFor\"] = srch_word\n",
    "    udict[\"protocol\"] = prot_dict[protocol]\n",
    "    for c in cat_table.Categorize:\n",
    "        udict[c] = \"(any)\"\n",
    "    if subcategory !=\"\": \n",
    "        udict[cat_table.get_value(cindx,\"Categorize\", takeable=False)] = subcategory\n",
    "    url_dict = udict.copy()\n",
    "    url_dict.update(val_dict)\n",
    "    data = urlencode(url_dict)\n",
    "    gen_url_html =  \"{}{}\".format(adv_url_html, data)\n",
    "    quote(gen_url_html, safe='') \n",
    "    gen_url_json = gen_url_html\n",
    "    gen_url_json = gen_url_json.replace(\".html\",\".json\")  \n",
    "    return gen_url_html, gen_url_json\n",
    "\n",
    "# Attempt url request, report error if encountered \n",
    "def url_request(url,rtype):\n",
    "    try:\n",
    "        if rtype == 'json':\n",
    "            r = requests.get(url).json()\n",
    "#            r.raise_for_status()\n",
    "        else:\n",
    "            r = requests.get(url, stream=True)\n",
    "#            r.raise_for_status()\n",
    "    except ValueError as e:  \n",
    "        sys.stdout.write(\"\\n{}{}\\n\\n{}\".format('** Flag ** : ', e,\"Search criteria could be too restrictive\"))\n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        sys.stdout.write(\"\\n{}\".format(\"\\n** Flag ** : system timeout - retry later\")) \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        sys.stdout.write(\"\\n** Flag ** : http request error\") \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        sys.stdout.write(\"\\n** Flag ** : systematic exception error\\nSearch url may need revision\") \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    else:\n",
    "        err = 'passed'\n",
    "    return r, err\n",
    "\n",
    "# evaluate whether file directory exists or intialize (read/create) download status file as json\n",
    "def ensure_dir(directory, url_flag, nf, task):\n",
    "    if task =='initialize':\n",
    "        file_sb = \"{}{}\".format(directory, sb_json)\n",
    "        if path.isfile(file_sb) and url_flag is not False:\n",
    "            with open(file_sb) as json_data:  \n",
    "                status = json.load(json_data)                \n",
    "            if 'json' in status[0]['general information'][5]['data search json']:\n",
    "                url_flag = True\n",
    "            else:\n",
    "                sys.stdout.write(\"\\nNote: adv. search url was not found in sb_json (download status) file\")\n",
    "                url_flag = False\n",
    "        else:\n",
    "            sys.stdout.write(\"\\nNote: sb_json (download status) file was not found in specified work folder\")\n",
    "            url_flag = False\n",
    "            \n",
    "        # if download status file is absent or disregarded then create, initialize, and overwrite new file   \n",
    "        if url_flag == False:\n",
    "            status = [{'general information': \"none\"},{'local_directory': tempdir},{'datasets (id)': {}}] \n",
    "            status[0]['general information'] = nf\n",
    "            status[0]['general information'][3]['data base url'] = baseurl \n",
    "            date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "            status[0]['general information'][1]['file created (date-time)'] = date\n",
    "            with open(file_sb, 'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "        return  status, url_flag\n",
    "    else:\n",
    "        try:\n",
    "            if not path.exists(directory):\n",
    "                makedirs(directory)\n",
    "        except:\n",
    "            return\n",
    "\n",
    "# Download files - data chunking option, uses content disposition   \n",
    "def download_file(url, fpath, altname, chunk):                   \n",
    "    try:\n",
    "        response, err = url_request(url,'file')                 \n",
    "    except:\n",
    "        sys.stdout.write(\"\\n\\tRequest error - url: %s\" % url)\n",
    "        download_file\n",
    "        return 'Request error - url'\n",
    "    if response.status_code == 200: \n",
    "        try:\n",
    "            d = response.headers['content-disposition']\n",
    "        except:\n",
    "            d = []\n",
    "            sys.stdout.write(\"\\n\\tGeneric filename using disposition:\\n\\t %s\" % url)\n",
    "        if d != []:\n",
    "            locf = ''.join(re.findall(\"filename=(.+)\", d))\n",
    "        else:\n",
    "            locf = altname \n",
    "        path = \"{}/{}\".format(fpath, locf)\n",
    "    \n",
    "        # Stream file object, no data chunking\n",
    "        if chunk == \"OFF\":\n",
    "            with open(path, 'wb') as f:\n",
    "                response.raw.decode_content = True\n",
    "                copyfileobj(response.raw, f)\n",
    "                sys.stdout.write(\"\\n\\tDownloaded %s \" % locf)\n",
    "                response.close()\n",
    "                return path    \n",
    "        # Chunk data - adjust iter_content size (bytes) ** -- not yet tested\n",
    "        else:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "                sys.stdout.write(\"\\n\\tDownloaded %s \" % locf)\n",
    "                response.close()\n",
    "                return path\n",
    "    else:\n",
    "        sys.stdout.write(\"\\n\\t%s returned request error status code: %s\" % (altname, response.status_code)) \n",
    "        response.close()\n",
    "        return\n",
    "    sys.stdout.write(\"\\nRequest error status code: %s\" % r.status_code)\n",
    "    \n",
    "     \n",
    "# Attempt retrieval of data files, metadata, and misc information\n",
    "# check information existence, purge (i.e. update) data files if commanded\n",
    "def data_proc(status, df_request, url_flag, purge_proc, purge_sb):\n",
    "    \n",
    "    # purge data files - reset download/upload process\n",
    "    if purge_proc == True:\n",
    "        sys.stdout.write(\"\\n** Purge requested reset file/metadata information\")\n",
    "        for index, row in df_request.iterrows():\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['download'] = 'incomplete'\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['upload'] = 'incomplete'\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['files'] = { k: \"None\" for k in (\n",
    "                dataproc + metafiles + ['info_dict','info_request'])}\n",
    "        with open(status[1]['local_directory'] + sb_json, 'w') as fp:\n",
    "            json.dump(status, fp, indent=4)  \n",
    "    elif purge_sb == 'reset':\n",
    "        for index, row in df_request.iterrows():\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['upload'] = 'incomplete'\n",
    "        with open(status[1]['local_directory'] + sb_json, 'w') as fp:\n",
    "            json.dump(status, fp, indent=4)  \n",
    "            \n",
    "    for index, row in df_request.iterrows():\n",
    "        \n",
    "        # ensure existence of working directory (temporary?) \n",
    "        fpath = \"{}{}\".format(tempdir, row[\"Dataset ID\"])\n",
    "        ensure_dir(fpath, '--', '--', '--')\n",
    "        \n",
    "        # if warranted create or modify dataset download record (ERDDAP dataset ID) \n",
    "        if row['Dataset ID'] not in status[2]['datasets (id)']:\n",
    "            status[2]['datasets (id)'][row['Dataset ID']] = OrderedDict([('sb_id',\"None\"), ('dataset_url', \"None\"), \n",
    "                         ('download','incomplete'), ('upload','incomplete'), ('files', OrderedDict())])\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['files'] = { k: \"None\" for k in (\n",
    "                                 dataproc + metafiles + ['info_dict','info_request'])}  \n",
    "  \n",
    "        # find ERDDAP dataset type (table, grid, wms), assumes type entries are unique \n",
    "        url_base = ''.join([row[\"tabledap\"], row[\"griddap\"], row[\"wms\"]])\n",
    "        status[2]['datasets (id)'][row['Dataset ID']]['dataset_url'] = url_base + \".html\"\n",
    "\n",
    "        # retrieve requested record information \n",
    "        if status[2]['datasets (id)'][row['Dataset ID']]['download'] != 'incomplete':\n",
    "            sys.stdout.write(\"\\n** Dataset ** : %s\\n\\n  files downloaded - bypassing download:\" % row['Dataset ID'])\n",
    "        else:\n",
    "            sys.stdout.write(\"\\n** Processing Dataset ** : %s - %s\\n\\nProcessing datafiles:\"\n",
    "                % (row[\"Dataset ID\"], row[\"Title\"]))\n",
    "\n",
    "            # retrieve requested ERDDAP datasets\n",
    "            for d in dataproc:\n",
    "                if status[2]['datasets (id)'][row['Dataset ID']]['files'][d] == \"None\":\n",
    "                    url = ''.join(\"{}{}\".format(url_base, d))\n",
    "                    altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d))\n",
    "                    f = download_file(url,fpath, altname, \"OFF\") \n",
    "                    status[2]['datasets (id)'][row['Dataset ID']]['files'][d] = f\n",
    "            sys.stdout.write(\"\\n\\tStep completed...\\n\")\n",
    "            \n",
    "            sys.stdout.write(\"\\nProcessing metafiles:\")\n",
    "            for d in metafiles:\n",
    "                if status[2]['datasets (id)'][row['Dataset ID']]['files'][d] == \"None\":\n",
    "                    url = ''.join(\"{}{}\".format(url_base, d))\n",
    "                    altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d)) \n",
    "                    f = download_file(url,fpath, altname, \"OFF\") \n",
    "                    status[2]['datasets (id)'][row['Dataset ID']]['files'][d] = f\n",
    "            sys.stdout.write(\"\\n\\tStep completed...\\n\")\n",
    "            \n",
    "            with open(status[1]['local_directory'] + sb_json, 'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "            # process data table information \n",
    "            sys.stdout.write(\"\\nProcessing table information:\")\n",
    "\n",
    "            # info dictionary \n",
    "            info_request = {}\n",
    "            info_dict = {'summary_info': {},'data_info' : {} }\n",
    "            \n",
    "            # retrieve other ERDDAP information as dictionary (info_request, RSS, or basic xml, etc.)\n",
    "            if status[2]['datasets (id)'][row['Dataset ID']]['files']['info_request'] == \"None\":\n",
    "                for d in table_info:\n",
    "                    entry = row[d]\n",
    "                    if \"http\" in entry and d != 'Summary': # search additional link to info\n",
    "                        ext = entry.rsplit(\".\",1)[1]\n",
    "                        sys.stdout.write(\"\\n\\tRetrieving additional url to request: %s \" % d)\n",
    "                        altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d))\n",
    "                        if d == \"Info\":\n",
    "                            info_request, err = url_request(entry,'json')\n",
    "                            if err != 'error':\n",
    "                                f = \"{}{}\".format(fpath,\"/info_request.json\")\n",
    "                                with open(f, 'w') as fp:\n",
    "                                    json.dump(info_request, fp, indent=4)\n",
    "                                status[2]['datasets (id)'][row['Dataset ID']]['files']['info_request'] = f\n",
    "                        elif d == 'RSS': # request, parse rss file\n",
    "                            response, err = url_request(entry,'file')  \n",
    "                            if response.status_code == 200:\n",
    "                                response.raw.decode_content = True\n",
    "                                tree = etree.parse(response.raw)\n",
    "                                root = tree.getroot()\n",
    "                                label = root.tag.rsplit(\"}\",1)[1]\n",
    "                                ns = {label: root.nsmap[None]}\n",
    "                                f = \"{}{}{}{}\".format(\"//\",label,\":\", 'item/*')\n",
    "                                modinfo = tree.xpath(f, namespaces=ns)\n",
    "                                info_dict['summary_info']['rss'] = {}\n",
    "                                for r in modinfo:\n",
    "                                    info_dict['summary_info']['rss'][r.tag.rsplit(\"}\",1)[1]] = r.text \n",
    "                                f= '//rss:pubDate'\n",
    "                                modinfo = tree.xpath(f, namespaces=ns)\n",
    "                                for r in modinfo:\n",
    "                                    info_dict['summary_info']['rss']['pubDate'] = r.text\n",
    "                            response.close()\n",
    "                        elif ext == \"xml\":  # request, convert xml request to dictionary\n",
    "                            response, err = url_request(entry,'file')  \n",
    "                            if response.status_code == 200:\n",
    "                                response.raw.decode_content = True\n",
    "                                tree = etree.parse(response.raw)\n",
    "                                xml_string = etree.tostring(tree)\n",
    "                                info_dict['data_info'][d] = xml_to_dict(xml_string)\n",
    "                            response.close()\n",
    "                        else:\n",
    "                            sys.stdout.write('file type: %s is not detailed.'\n",
    "                                             '\\nBulk printing response as string' % ext ) \n",
    "                            response, err = url_request(entry,'file') \n",
    "                            if response.status_code == 200:\n",
    "                                info_dict['data_info'][d] = response.text\n",
    "                            response.close()\n",
    "                    else:\n",
    "                        info_dict['summary_info'][d] = entry # copy text block from data table\n",
    "\n",
    "                    status[2]['datasets (id)'][row['Dataset ID']]['download'] = 'attempted'\n",
    "            \n",
    "                # update file information\n",
    "                f = \"{}{}\".format(fpath, \"/info_dict.json\")\n",
    "                with open(f, 'w') as fp:\n",
    "                    json.dump(info_dict, fp, indent=4)\n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['files']['info_dict'] = f\n",
    "            \n",
    "            # check if downloads complete\n",
    "            qc_files = True\n",
    "            bstat = status[2]['datasets (id)'][row['Dataset ID']]['files']\n",
    "            for key, value in bstat.iteritems():\n",
    "                if value == \"None\":\n",
    "                    qc_files = False\n",
    "            if qc_files == True:     \n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['download'] = \"YES\"\n",
    "            else:\n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['download'] = 'request/processing errors' \n",
    "            with open(status[1]['local_directory'] + sb_json, 'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "        sys.stdout.write(\"\\n\\tStep completed...\\n\")         \n",
    "        \n",
    "        # work with only one dataset = for testing purposes - comment out break otherwise      \n",
    "        # break\n",
    "    return\n",
    "\n",
    "# Search ERDDAP datasets using a simple interface (otherwise get adv url and bypass)  - incomplete ** \n",
    "def search_url(df, bulk_opts_dict, retries=5, complaint='Invalid entries - stopped'):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(\"Request ERDDAP datasets using a retrieval option <letter>: \\n\\t%s\\n\\t%s\" \n",
    "                  % (\"(B) Bulk type constraint, or\", \"(S) Search (categorical, keyword(s), phrase)\"))\n",
    "        entry = (raw_input()).upper()\n",
    "        \n",
    "        if entry == \"B\":  # bulk file read \n",
    "            while True:\n",
    "                sys.stdout.write(\"\\nRequest all: (D) datasets, (T) Tables, or (G) Grids\\n\")\n",
    "                bulk = (raw_input()).upper()\n",
    "                if bulk in bulk_opts_dict:\n",
    "                    link = ''.join(bulk_opts_dict[bulk][2]).replace(\".json\",\".html\")\n",
    "                    print \"\\nBulk Request %s\\nURL: %s\" % ( bulk_opts_dict[bulk][1], link )\n",
    "                    response, err = url_request(bulk_opts_dict[bulk][2],'json')\n",
    "                    df_query = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                    get_stats(df_query)\n",
    "                    return df_query, link, bulk_opts_dict[bulk][2] \n",
    "                else:\n",
    "                    print (\"\\nInvalid command - enter an indicated letter option (*)\") \n",
    "        \n",
    "        elif entry == \"S\":  # Search by category, search word(s), and/or protocol (data type)\n",
    "            sys.stdout.write(\"\\n*** Note: custom search methods are loosely fitted - in progress ***\")\n",
    "            response, err = url_request(bulk_opts_dict[\"C\"][2],'json')\n",
    "            df_cquery = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "            df_cquery.columns.name = \"Search Index\"\n",
    "            display(df_cquery)\n",
    "            \n",
    "            # Category search\n",
    "            sys.stdout.write(\"Select search index for category [left column] (optional: # is not index --> skip):\\n\")\n",
    "            try:\n",
    "                cindx = int(raw_input())\n",
    "            except:\n",
    "                cindx = [] \n",
    "            squery_entry = \"\"\n",
    "            dfl = list(df_cquery.index.values)\n",
    "            if cindx in dfl:\n",
    "                url = ''.join(df_cquery.iloc[[cindx]][\"URL\"])\n",
    "                response, err = url_request(url,'json')\n",
    "                df_squery = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                df_squery.columns.name = \"Search Index\"\n",
    "                display(df_squery)\n",
    "                sindx = raw_input(\"Select search index for subcategory [left column] (optional: # is not index --> skip):\\n\")\n",
    "                try:\n",
    "                    sindx = int(sindx)\n",
    "                except:\n",
    "                    sindx = []\n",
    "                dfl = list(df_squery.index.values)\n",
    "                if sindx in dfl:\n",
    "                    squery_entry = df_squery.get_value(sindx,\"Category\", takeable=False)\n",
    "                    url = ''.join(df_squery.iloc[[sindx]][\"URL\"])\n",
    "                    response, err = url_request(url,'json')\n",
    "                    df_query = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])  \n",
    "                else:\n",
    "                    sys.stdout.write(\"\\n\\tEntered search index was not in list, skipped subcategory search.\\n\\n\")       \n",
    "            else:\n",
    "                sys.stdout.write(\"\\n\\tEntered search index was not in list, skipped category refinement.\\n\\n\")   \n",
    "\n",
    "            # Word or phrase search \n",
    "            sys.stdout.write(\"Optional keyword or phrase search\\nEnter space-delimited search word(s) or qouted phrase (blank --> skip):\\n\")\n",
    "            search = raw_input()\n",
    "            if search == '': \n",
    "                sys.stdout.write(\"Search input was blank, skipped word search\")  \n",
    "            \n",
    "            # Protocol (data type) search constraint - currently does not examine subsets\n",
    "            sys.stdout.write(\"\\nDefine allowed protocol (data type): \\n\\t%s\\n\\t%s\\n\\t%s\\n\" \n",
    "                  % (\"(A) All data types,\", \"(G) Griddap, or\", \"(T) Tabledap, or (W) Wms (default --> All types)\"))\n",
    "            protocol = (raw_input()).upper()\n",
    "            if protocol not in [\"A\", \"G\", \"T\", \"W\"]: protocol = \"A\"\n",
    "            \n",
    "            # Gather pertinent info, build custom url for http services\n",
    "            comb = ''.join([search, protocol, squery_entry])\n",
    "            if comb != \"A\":\n",
    "                gen_url_html, gen_url_json = build_url(search, protocol, df_cquery, cindx, squery_entry)   \n",
    "                sys.stdout.write(\"\\n'Advanced' search url:\\n%s\\n\" % gen_url_html)\n",
    "                try:\n",
    "                    url_request(gen_url_json,'json')\n",
    "                except:\n",
    "                    sys.stdout.write(\"Identified exception during url request\")\n",
    "                    return\n",
    "                df_request = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                get_stats(df_request)        \n",
    "                return df_request, gen_url_html, gen_url_json\n",
    "            else:\n",
    "                sys.stdout.write(\"\\nNo search constraints were detected - retry\\n\\n\")  \n",
    "        else:     \n",
    "            print (\"\\nInvalid command - enter a letter option (*)\\n\") \n",
    "\n",
    "# set permissions\n",
    "def set_permissions(item_id, acls):\n",
    "    sb_base_url = \"https://www.sciencebase.gov/catalog/item/\" + item_id + \"/permissions/\"\n",
    "    return sb._get_json(sb._session.put( sb_base_url, data=json.dumps(acls)))\n",
    "\n",
    "# adjust read/write (task) privleges \n",
    "def set_acls(acls, names, task):\n",
    "    if 'inheritsFromId' in acls[task]:\n",
    "        del acls[task]['inheritsFromId']\n",
    "    acls[task]['inherited'] = False\n",
    "    new_acls = []\n",
    "    for p in names:\n",
    "        new_acls.append(p)\n",
    "    acls[task]['acl'] = new_acls\n",
    "    return acls\n",
    "\n",
    "# reformat date entries\n",
    "def date_reform(date_input):\n",
    "    date_rec = str(parser.parse(date_input, ignoretz=True)).split(\" \",1)[0]\n",
    "    if date_rec == datetime.date.today():\n",
    "        date_rec = \"{}{}\".format(date_input,'_recheck') \n",
    "    return date_rec\n",
    "\n",
    "# pull info from ERDDAP info table as pandas dataframe (adhoc corrections for now, work in progress)\n",
    "def populate_sbase(attribute , rmv, info_frame):\n",
    "    indx = info_frame[info_frame['Attribute Name'] == attribute].index.tolist()[0]\n",
    "    entry = info_frame.get_value(indx,\"Value\", takeable=False)\n",
    "    if rmv == 'yes':\n",
    "        if isinstance(entry, basestring):\n",
    "            entry = re.sub(r'\\,(?! )', ', ', re.sub(r' +', ' ', entry)) # ensure space after commas\n",
    "#            entry = re.sub(r'\\.(?! )', '. ', re.sub(r' +', ' ', entry)) # ensure space after period\n",
    "            entry = entry.rstrip() # remove carriage returns /n  \n",
    "            entry = entry.replace('?s',\"'s\") # remove erroneous \"?\" unicode trans errors\n",
    "    return entry\n",
    "\n",
    "# Create and update SB records, modify as needed\n",
    "def SciBase_item(status, nf_dict, purge_sb):\n",
    "\n",
    "    # login to Sciencebase\n",
    "    sys.stdout.write(\"{}{}\".format('\\n** Processing ScienceBase items **','\\n\\n\\tLogin to ScienceBase - '))\n",
    "    sb = pysb.SbSession()\n",
    "    sb.loginc(str(login_name))\n",
    "    time.sleep(5)\n",
    "\n",
    "    # iterate over list of datasets in search record\n",
    "    for dataset in status[2]['datasets (id)']:\n",
    "\n",
    "        # individual dataset download information\n",
    "        data = status[2]['datasets (id)'][dataset]\n",
    "        \n",
    "        if data[\"download\"] == 'YES':\n",
    "\n",
    "            # Open ERDDAP dataset dictionary files (info_request, info_dict: *.json format)\n",
    "            with open(data['files']['info_request']) as fp:    \n",
    "                info_request = json.load(fp)\n",
    "            info_frame = pd.DataFrame(info_request['table']['rows'], columns=info_request['table']['columnNames'])\n",
    "            with open(status[2]['datasets (id)'][dataset]['files']['info_dict']) as fp:    \n",
    "                info_dict = json.load(fp)\n",
    "\n",
    "            sys.stdout.write(\"\\nProcessing dataset: %s into ScienceBase\\n\" % dataset)\n",
    "\n",
    "            # check SB item existence (visibility)\n",
    "            chk = True\n",
    "            if data[\"sb_id\"] != \"None\":\n",
    "                try:\n",
    "                    SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "                except:\n",
    "                    sys.stdout.write(\"\\n{}{}{}\".format('Warning: SB item ', data[\"sb_id\"],' was NOT located - creating new item'))\n",
    "                    chk = False\n",
    "                else:\n",
    "                    sys.stdout.write(\"\\n\\t{}{}{}\".format('SB item: ', data[\"sb_id\"],' was located in ScienceBase'))\n",
    "            else:\n",
    "                chk = False\n",
    "            \n",
    "            # create new sciencebase item, if warranted\n",
    "            if chk == False:\n",
    "                new_item_info = {'title': populate_sbase(\"title\", 'yes', info_frame),\n",
    "                    'parentId': sb.get_my_items_id(),\n",
    "                    'provenance': {'annotation': nf_dict['prov_state']}}\n",
    "                SB_rec = sb.create_item(new_item_info)\n",
    "                data[\"sb_id\"] = SB_rec['id']\n",
    "                status[2]['datasets (id)'][dataset][\"sb_id\"] = SB_rec['id']\n",
    "                sys.stdout.write(\"\\n\\t{}{}\".format('Creating new ScienceBase item:', data[\"sb_id\"]))\n",
    "                with open(tempdir + sb_json,'w') as fp:    \n",
    "                    json.dump(status, fp, indent=4)\n",
    "            \n",
    "            # check SB item permissions, if no permissions create new item\n",
    "            if u'USER:' + login_name in SB_rec['permissions'][u'write'][u'acl']:\n",
    "                sys.stdout.write(\"\\n\\t** Verified ** user has write privileges for item\")\n",
    "            else:\n",
    "                sys.stdout.write(\"\\n *****  WARNING  ***** : user without write privileges - creating new item\")\n",
    "                data[\"sb_id\"] = \"None\"\n",
    "                new_item_info = {'title': populate_sbase(\"title\",'yes', info_frame),\n",
    "                    'parentId': sb.get_my_items_id(),\n",
    "                    'provenance': {'annotation': nf_dict['prov_state']}}\n",
    "                SB_rec = sb.create_item(new_item_info)\n",
    "\n",
    "            sys.stdout.write(\"\\n\\tCheck files, upload/update files and metadata\")\n",
    "            \n",
    "            # gather list of files currently in item's 'files' and 'facets'\n",
    "            uploaded_files = []\n",
    "            if 'files' in SB_rec:\n",
    "                uploaded_files = [f_ex['name'] for f_ex in SB_rec['files']]  \n",
    "            if 'facets' in SB_rec:\n",
    "                for facet in SB_rec['facets']:\n",
    "                    if 'files' in facet:\n",
    "                        for f in facet['files']:\n",
    "                            uploaded_files.append(f['name'])\n",
    "                        \n",
    "            # upload only new files or upload/update all files\n",
    "            if purge_sb != True:\n",
    "                for f in (metafiles + dataproc):\n",
    "                    fn = data['files'][f].rsplit( \"/\", 1 )[ 1 ] \n",
    "                    if fn not in uploaded_files:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"uploading new file to item\"))\n",
    "                        sb.uploadFileToItem(SB_rec, data['files'][f])\n",
    "            elif data[\"upload\"] != \"YES\":\n",
    "                for f in (metafiles + dataproc):\n",
    "                    fn = data['files'][f].rsplit( \"/\", 1 )[ 1 ] \n",
    "                    if fn not in uploaded_files:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"uploading new file to item\"))\n",
    "                        sb.uploadFileToItem(SB_rec, data['files'][f])\n",
    "                    else:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"updating existing file in item\"))\n",
    "                        sb.replace_file(data['files'][f], SB_rec)\n",
    "                        SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "                   \n",
    "\n",
    "            sys.stdout.write(\"\\n\\tModifying ScienceBase record information\")        \n",
    "                    \n",
    "            # retrieve latest SB item after file uploads/updates \n",
    "            SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "                        \n",
    "            # record SB item create date \n",
    "            entry_create = date_reform(populate_sbase(\"date_created\",'no', info_frame))\n",
    "\n",
    "            # data record start date\n",
    "            entry_start = date_reform(populate_sbase(\"time_coverage_start\",'no', info_frame))\n",
    "\n",
    "            # data record end date\n",
    "            entry_end = date_reform(populate_sbase(\"time_coverage_end\",'no', info_frame))\n",
    "\n",
    "            # files retrieval (download) date using OS call for metadata file\n",
    "            date_input = path.getmtime(data['files'][metafiles[0]])\n",
    "            Int2date = datetime.datetime.fromtimestamp(date_input)\n",
    "            retrieve = datetime.datetime.strftime(Int2date, '%Y-%m-%d')\n",
    "            \n",
    "            # load data record dates  \n",
    "            rep = status[0]['general information'][2]['file modified (date-time)']\n",
    "            if rep == 'none' or 'dates' not in SB_rec:                   \n",
    "                SB_rec['dates']  = [{u\"type\":u\"Item submission\",\n",
    "                    u\"dateString\":str(datetime.date.today()), u\"label\": u\"Item submission\"}]       \n",
    "            else:\n",
    "                SB_rec[u'dates'] = [SB_rec[u'dates'][0]]\n",
    "                \n",
    "            SB_rec['dates'].extend([\n",
    "                {u\"type\":u\"Data record (start)\",\n",
    "                u\"dateString\":entry_start,\n",
    "                u\"label\":u\"Data record (start)\"},\n",
    "                {u\"type\":u\"Data record (end)\",\n",
    "                u\"dateString\":entry_end,\n",
    "                u\"label\":u\"Data record (end)\"},\n",
    "                {u\"type\":u\"Retrieved from source\",\n",
    "                u\"dateString\":str(retrieve),\n",
    "                u\"label\":u\"Retrieved from source\"}])\n",
    "                                 \n",
    "            # update provenance\n",
    "            SB_rec['provenance']['annotation'] = nf_dict['prov_state'] \n",
    "\n",
    "            # update citation information, if available\n",
    "            try:\n",
    "                SB_rec[\"citation\"] = \"{}{}\".format(\"{}{}\".format(\"Data citation - \", populate_sbase(\"bibliographicCitation\",'yes', info_frame)),\n",
    "                    \"{}{}\".format(\". Study citation - \", populate_sbase(\"associatedReferences\",'yes', info_frame)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # update title\n",
    "            SB_rec[\"title\"] = populate_sbase(\"title\",'yes', info_frame)\n",
    "\n",
    "            # update additional information\n",
    "            entry_license = populate_sbase(\"license\",'yes', info_frame)\n",
    "\n",
    "            # update summary + add license information\n",
    "            SB_rec[\"body\"] = \"{}{}{}\".format(populate_sbase(\"summary\",'yes', info_frame),'&nbsp; &nbsp;\\n<br> \\n<br>',entry_license)\n",
    "\n",
    "            # provide OPeNDAP weblinks\n",
    "            webrec = []\n",
    "            for w in webnames:\n",
    "                d_url = status[2]['datasets (id)'][dataset][\"dataset_url\"].replace('.html', w)\n",
    "                rec = {\n",
    "                    u'hidden': False,\n",
    "                    u'rel': u'related',\n",
    "                    u'title': file_dict[w],\n",
    "                    u'type': u'OPeNDAP weblinks - data provenance trace',\n",
    "                    u'uri': d_url} \n",
    "                webrec.append(rec)\n",
    "            SB_rec['webLinks'] = webrec\n",
    "            \n",
    "            # edit and remove redundancy in contact names\n",
    "            store_name = []\n",
    "            if SB_rec['contacts']:\n",
    "                for s in SB_rec['contacts']:\n",
    "                    if s['name'] != '' and '?' not in s['name']:\n",
    "                        store_name.append(s)\n",
    "            if nf_dict['BCB_contact']['name'] not in store_name:\n",
    "                store_name.append(nf_dict['BCB_contact'])\n",
    "            if nf_dict['processor']['name'] not in store_name:\n",
    "                store_name.append(nf_dict['processor'])\n",
    "            if nf_dict['publisher']['name'] not in store_name:\n",
    "                store_name.append(nf_dict['publisher'] )\n",
    "            seen = set()            \n",
    "            new_store_name = []\n",
    "            for s in store_name:\n",
    "                if s['name']  not in seen:\n",
    "                    seen.add(s['name'])\n",
    "                    new_store_name.append(s) \n",
    "            SB_rec['contacts'] = new_store_name\n",
    "\n",
    "            # set permissions \n",
    "            acls = SB_rec['permissions']\n",
    "            set_acls(acls, nf_dict['read_names'],'read')\n",
    "            set_acls(acls, nf_dict['write_names'],'write')\n",
    "            # set_permissions(SB_rec['id'], acls) - currently not used (sub 2 lines below)\n",
    "            sb_base_url = \"https://www.sciencebase.gov/catalog/item/\" + SB_rec['id'] + \"/permissions/\"\n",
    "            sb._get_json(sb._session.put( sb_base_url, data=json.dumps(acls)))\n",
    "\n",
    "            # remove redundancy in tag names\n",
    "            seen = set()\n",
    "            new_tags = []\n",
    "            for d in SB_rec[u'tags']:\n",
    "                if d['name']  not in seen:\n",
    "                    seen.add(d['name'])\n",
    "                    new_tags.append(d) \n",
    "            SB_rec[u'tags'] = new_tags\n",
    "\n",
    "            # save updates, file\n",
    "            sys.stdout.write(\"\\n\\tScienceBase processing completed\\n\")\n",
    "            sb.updateSbItem(SB_rec)\n",
    "            status[2]['datasets (id)'][dataset][\"upload\"] = \"YES\"\n",
    "            with open(tempdir + sb_json,'w') as fp:    \n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "            # work with only one dataset = for testing purposes - comment out (break below) otherwise       \n",
    "            # break \n",
    "        else:\n",
    "            sys.stdout.write(\"\\nInfo for dataset %s - incomplete not processed\\n\" % dataset)  \n",
    "    \n",
    "# *****************************************************\n",
    "\n",
    "#   code below started, greater work in progress\n",
    "                         \n",
    "#   Generate input/log file of manual search, for re-runs and reproduction - NOT yet implemented **\n",
    "#   Add Header username, computer info, datestamp add logging capabilities\n",
    "#   class Logger(object):\n",
    "#      def __init__(self, filename=\"Default.log\"):\n",
    "#          self.terminal = sys.stdout\n",
    "#          self.log = open(filename, \"a\")\n",
    "#\n",
    "#      def write(self, message):\n",
    "#          self.terminal.write(message)\n",
    "#          self.log.write(message)\n",
    "#   sys.stdout = Logger(\"yourlogfilename.txt\")\n",
    "\n",
    "# Request upper-level info on available datasets incomplete **\n",
    "# def reqtable(df, name):\n",
    "#    global df_result\n",
    "#    url = ''.join(df.loc[df['Resource'] == name]['URL'])\n",
    "#    response, err = url_request(url,'json')\n",
    "#    df_result = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames']) \n",
    "#    df_result.drop(df_result[df_result['Dataset ID'] == \"allDatasets\"].index)\n",
    "#    get_stats(df_result)      \n",
    "#    return\n",
    "    \n",
    "# Evaluate table of requested datasets, flag or exclude datasets as warranted.  incomplete - not started **\n",
    "# def evaltable(df_result):\n",
    "#    print(\"... incomplete, may perform assessments in the future\")\n",
    "#    return\n",
    "\n",
    "# Evaluate individual datasets, flag or exclude as warranted, incomplete - not started **\n",
    "# def evaldataset(df_result):\n",
    "#    print(\"Functionality may include QA/QC metrics, taxonomic checks, and other criteria\")\n",
    "#    return\n",
    "\n",
    "print('definitions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "# *******************************************************\n",
    "#                   Main Program: \n",
    "# *******************************************************\n",
    "\n",
    "def main(): \n",
    "    \n",
    "    tstamp = []\n",
    "    tasklist = []\n",
    "    tasklist.append('Main program processing')\n",
    "    tstamp.append(timer())\n",
    "    \n",
    "    sys.stdout.write('Search ERDDAP data repository, develop search criteria\\n') \n",
    "    \n",
    "    # open name/permissions information (generated a priori)\n",
    "    with open(tempdir + \"name_file_dict.json\") as fp:    \n",
    "        nf_dict = json.load(fp)\n",
    "    \n",
    "    # check existence of download status file, adv. search url\n",
    "    SB_status, url_chk = ensure_dir(tempdir, url_flag, nf_dict[ u'general information'], 'initialize') \n",
    "        \n",
    "    if url_chk is not True:\n",
    "        \n",
    "        # ERDDAP general data resources \n",
    "        subdir   = '/index'  \n",
    "        url_o = \"\".join((baseurl, subdir, infotype))\n",
    "        sys.stdout.write(\"\\nHome URL: %s \\n\\n\" % (url_o.rsplit( \".\", 1 )[ 0 ] + linktype )) \n",
    "        response, err = url_request(url_o,'json')\n",
    "        \n",
    "        if dataform == 'dframe':\n",
    "            \n",
    "    #      pandas dataframe visual specs\n",
    "            pd.set_option('display.notebook_repr_html', True)\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            pd.set_option('display.max_rows', 500)\n",
    "            colwidth = 100 # maximum column width in final request table\n",
    "            maxrows = 200 # maximum datasets to show in final request table\n",
    "\n",
    "    #      Main navigation links of the ERDDAP server\n",
    "            df = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "            df['URL'] = df['URL'].str.replace('.json', linktype)\n",
    "            sys.stdout.write(df.to_string(index=False, justify='left') + '\\n')\n",
    "\n",
    "    #      All datasets served by the ERDDAP server\n",
    "            url = ''.join(df.loc[df['Resource'] == 'info']['URL'])\n",
    "            sys.stdout.write(\"\\nLink to all datasets with metadata: %s\\n\" % url) \n",
    "\n",
    "    #      All datasets served by the ERDDAP server\n",
    "            url = \"\".join((baseurl, \"/tabledap/allDatasets\", linktype))\n",
    "            sys.stdout.write(\"\\nLink to all tabes, includes searchable information: %s\\n\\n\" % url) \n",
    "\n",
    "    #      Adjust links,settings\n",
    "            pd.set_option('display.max_colwidth',250)\n",
    "            df['URL'] = df['URL'].str.replace(linktype,'.json')\n",
    "\n",
    "    #      Set up info for search types (option, task, url)\n",
    "            bulk_opts_dict = bulkoption(df)\n",
    "\n",
    "        else: # full json method not yet developed, incomplete **\n",
    "            sys.stdout.write(\"{}{}\".format('Invalid entry, defaulting to .json format', '\\n')) \n",
    "            dataform == 'json'\n",
    "            pprint.pprint(response.keys())\n",
    "            pp = pprint.PrettyPrinter(indent=2)\n",
    "            sys.exit()\n",
    "    #       add other functions using json\n",
    "\n",
    "        # develop request table\n",
    "        tstamp.append(timer())\n",
    "        tasklist.append('Search and request datasets from ERDDAP')\n",
    "        df_request, gen_url_html, gen_url_json = search_url(df, bulk_opts_dict)\n",
    "        \n",
    "        # store ERDDAP advanced search url and save download status file\n",
    "        SB_status[0]['general information'][4]['data search url'] =  gen_url_html\n",
    "        SB_status[0]['general information'][5]['data search json'] =  gen_url_json\n",
    "        \n",
    "    else:\n",
    "        sys.stdout.write(\"\\n{}{}{}\\n\\n\".format('\\n*** Automated search using information file: ', sb_json,'  ***'))                \n",
    "        response, err  = url_request(SB_status[0]['general information'][5]['data search json'],'json')\n",
    "        if err == 'failed': \n",
    "            sys.stdout.write(\"Identified problem using previous search url (*.json)\\nExiting program\")\n",
    "            sys.exit()\n",
    "        else:\n",
    "            df_request = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "    \n",
    "    # process data - incomplete, work in progress\n",
    "    tstamp.append(timer())\n",
    "    tasklist.append('Process datasets and information')\n",
    "    data_proc(SB_status, df_request, url_flag, purge_proc, purge_sb)\n",
    "    \n",
    "    # create, set up, modify Sciencebase records for data search, work in progress\n",
    "    tstamp.append(timer())\n",
    "    tasklist.append('Develop and/or modify ScienceBase records')\n",
    "    sys.stdout.write(\"{}{}\".format('\\n** Processing ScienceBase items **','\\n\\n\\tLogin to ScienceBase - '))\n",
    "    if purge_sb != False:\n",
    "        SciBase_item(SB_status, nf_dict, True)\n",
    "    else:\n",
    "        SciBase_item(SB_status, nf_dict, False)\n",
    "    \n",
    "    # final save status file\n",
    "    date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "    SB_status[0]['general information'][2]['file modified (date-time)'] = date\n",
    "    with open(SB_status[1]['local_directory'] + sb_json, 'w') as fp:\n",
    "        json.dump(SB_status, fp, indent=4)\n",
    "    \n",
    "    tstamp.append(timer())\n",
    "    for i in range(len(tstamp)-1):\n",
    "        sys.stdout.write('\\nTask '+ str(i+1) + ': ' + tasklist[i] + ' | Runtime [secs]: '+ str(round(tstamp[i+1]-tstamp[i],3)))\n",
    "    \n",
    "    sys.stdout.write(\"\\n\\nProgram has finished\")\n",
    "\n",
    "    \n",
    "# ************************************************************************* \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "# ************************************************************************\n",
    "\n",
    "#  *** General inputs *** (place in input file at some point)\n",
    "\n",
    "#  username to login to ScienceBase \n",
    "    login_name = 'twellman@usgs.gov'\n",
    "\n",
    "#  Set local (working) directory + folders, eventually use full memory stream (note: '/' at end)\n",
    "    tempdir = \"/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/erddap_MBON_test/\"\n",
    "    \n",
    "#  Name of download status file (json), existing in local directory or to set new\n",
    "    sb_json = \"file_sb.json\"\n",
    "                         \n",
    "#  Base ERDDAP url - to \"/erddap\" level\n",
    "    baseurl  = 'http://gcoos4.tamu.edu:8080/erddap'\n",
    "    \n",
    "#  Content retrieval requests, includes datasets, metadata, and other information \n",
    "    dataproc = [\".nc\",\".ncHeader\", \".csv\"]    \n",
    "    metafiles =  [\".iso19115\"]\n",
    "    table_info =   [\"Dataset ID\", \"Title\", \"Institution\" ,\"Summary\", \"Background Info\",\"Info\",'RSS']\n",
    "    infoformat = \".json\"\n",
    "    \n",
    "#  Sciencebase OpenDap weblinks to show in ScienceBase item\n",
    "    webnames = ['.html','.csv','.nc','.ncHeader','.json','.fgdc','.geoJson','.iso19115']\n",
    "                         \n",
    "#  File format flags - hardwired, work in progress\n",
    "    infotype = '.json'   # format to read dataset information \n",
    "    linktype = '.html'   # format to show data resources\n",
    "    dataform = 'dframe'  # format to display data (Pandas dataframe) \n",
    "\n",
    "#  whether to use existing search (advanced search url) in \"sb_json\" file (above), \n",
    "#  if false/absent goes through simplfied query process and internally generates \n",
    "#  new adv url, can also manually create the general information section in \"sb_json\".\n",
    "    url_flag = True\n",
    "    \n",
    "# whether to overwrite content used in overall processing (proc) or ScienceBase (sb)\n",
    "# only ONE run for purge_proc = True, then set to False, else will always reset\n",
    "# keep purge_sb = True for purging ScienceBase items until complete\n",
    "# purge_sb can also be set to 'reset' to reset upload status to ScienceBase separately\n",
    "    purge_proc = False\n",
    "    purge_sb = False\n",
    "    \n",
    "# ************************************************************************\n",
    "    \n",
    "# call to main                                                 \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "***************************************************************************************************************\n",
    "******   \" The section below has ScienceBase code snippets to create, modify, retrieve information \"     ******\n",
    "***************************************************************************************************************\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# login to ScienceBase using pysb. retrieve item\n",
    "\n",
    "sb = pysb.SbSession()\n",
    "sb.loginc(str(login_name))\n",
    "time.sleep(5)\n",
    "test = sb.get_item('587f8830e4b085de6c11f242')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple print out a list of completed file downloads\n",
    "# from sb_json file. At present tweak adhoc for print out\n",
    "# includes hyperlinks (can cut and paste into doc)\n",
    "\n",
    "with open(tempdir + sb_json) as fp:    \n",
    "        SB_status = json.load(fp)\n",
    "    \n",
    "dataset = 0\n",
    "for d in SB_status[2][\"datasets (id)\"]:\n",
    "    if SB_status[2][\"datasets (id)\"][d][\"sb_id\"] != \"None\":\n",
    "        dataset = dataset  + 1\n",
    "        print(\"Dataset number: \", dataset)\n",
    "        repl = SB_status[2][\"datasets (id)\"][d]['dataset_url'] \n",
    "        SB_status[2]['datasets (id)'][d]['dataset_url'] = repl\n",
    "        print \"Dataset name:\", d\n",
    "        print \"ScienceBase ID: \", SB_status[2][\"datasets (id)\"][d][\"sb_id\"]\n",
    "        print \"ScienceBase url:\", \"https://www.sciencebase.gov/catalog/item/\" + SB_status[2][\"datasets (id)\"][d][\"sb_id\"]\n",
    "        print \"dataset_url: \", repl\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view pandas dataframe of table of datasets requested\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "info_frame = pd.DataFrame(info_request['table']['rows'], columns=info_request['table']['columnNames'])\n",
    "    \n",
    "display(info_frame) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# *******************************************************\n",
    "#                   Name Dictionary: \n",
    "# *******************************************************\n",
    "\n",
    "# Create a label dictionary to populate sciencebase items and header of download status file\n",
    "# modify as needed, specify path to save (tempdir) as working directory\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "name_file_dict = {}\n",
    "name_file_dict[u'prov_state'] = 'GCOOS MBON data retrieve'\n",
    "name_file_dict[u'write_names'] = [u'USER:twellman@usgs.gov', u\"USER:albenson@usgs.gov\", u\"USER:sbristol@usgs.gov\", u\"USER:saulenbach@usgs.gov\"]\n",
    "name_file_dict[u'read_names'] = [u'USER:twellman@usgs.gov', u\"USER:albenson@usgs.gov\", u\"USER:sbristol@usgs.gov\", u\"USER:saulenbach@usgs.gov\"]\n",
    "name_file_dict[u'publisher'] = {u'contactType': u'organization',\n",
    "       u'name': u'Marine Biodiversity Oservation Network',\n",
    "       u'onlineResource': u'http://oceanservice.noaa.gov/news/apr16/mbon.html',\n",
    "       u'organization': {u'displayText': u'Marine Biodiversity Oservation Network'},\n",
    "       u'primaryLocation': {},\n",
    "       u'type': u'Publisher'}\n",
    "name_file_dict[u'processor'] = {u'contactType': u'organization',\n",
    "       u'name': u'GCOOS: Gulf of Mexico Coastal Ocean Observing System',\n",
    "       u'onlineResource': u'http://gcoos.tamu.edu',\n",
    "       u'organization': {u'displayText': u'GCOOS: Gulf of Mexico Coastal Ocean Observing System'},\n",
    "       u'primaryLocation': {},\n",
    "       u'type': u'Processor'}\n",
    "name_file_dict[u'BCB_contact'] = {u'active': True,\n",
    "      u'contactType': u'person',\n",
    "      u'email': u'albenson@usgs.gov',\n",
    "      u'firstName': u'Abigail',\n",
    "      u'jobTitle': u'Biologist',\n",
    "      u'lastName': u'Benson',\n",
    "      u'middleName': u'L',\n",
    "      u'name': u'Abigail L Benson',\n",
    "      u'oldPartyId': 23179,\n",
    "      u'organization': {u'displayText': u'Scientific Data Integration and Visualization'},\n",
    "      u'primaryLocation': {u'building': u'DFC Bldg 810',\n",
    "      u'buildingCode': u'KBT',\n",
    "      u'faxPhone': u'3032024229',\n",
    "      u'mailAddress': {u'city': u'Denver',\n",
    "      u'country': u'USA',\n",
    "      u'line1': u'Box 25046, Denver Federal Center, Mail Stop 306',\n",
    "      u'mailStopCode': u'306',\n",
    "      u'state': u'CO',\n",
    "      u'zip': u'80225-0046'},\n",
    "      u'name': u'Abigial L Benson/GIO/USGS/DOI - Primary Location',\n",
    "      u'streetAddress': {u'city': u'Lakewood',\n",
    "      u'country': u'US',\n",
    "      u'line1': u'West 6th Ave. & Kipling St., DFC Bldg. 810',\n",
    "      u'state': u'CO',\n",
    "      u'zip': u'80225-0046'}},\n",
    "      u'type': u'Point of Contact'}\n",
    "name_file_dict[u'general information'] = [\n",
    "                {u'purpose' : u'Data Distillery development: MBON (GCOOS) data downloads'},\n",
    "                {u'file created (date-time)' : 'none'},\n",
    "                {u'file modified (date-time)' : 'none'},\n",
    "                {u'data base url': 'none'},\n",
    "                {u'data search url': ''},\n",
    "                {u'data search json': ''},\n",
    "                {u'contacts': u'Abigail Benson, Tristan P. Wellman, Steve Aulenbach, Sky Bristol'},\n",
    "                {u'email': u'albenson@usgs.gov, twellman@usgs.gov, saulenbach@usgs.gov, sbristol@usgs.gov'},\n",
    "                {u'organization':u'U.S. Geological Survey, Core Science Analytics, and Synthesis (B.C.B.)'},\n",
    "                {u'address': u'Denver Federal Center, Building 810, Lakewood, Colorado'}]  \n",
    "\n",
    "#  Save name file to local directory - incomplete\n",
    "\n",
    "tempdir = \"/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/ERDDAP_MBON_update_test/\"\n",
    "\n",
    "with open(tempdir + \"name_file_dict.json\", 'w') as fp:\n",
    "    json.dump(name_file_dict, fp, indent=4)\n",
    "pprint.pprint(name_file_dict)\n",
    "\n",
    "#with open(tempdir + \"name_file_dict.json\") as fp:    \n",
    "#    nf_dict = json.load(fp)\n",
    "#pprint.pprint(nf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fiddling with Asynchronous HTTP client\n",
    "\n",
    "from tornado import ioloop, httpclient\n",
    "\n",
    "i = 0\n",
    "\n",
    "def handle_request(response):\n",
    "    print(response.code)\n",
    "    global i\n",
    "    i -= 1\n",
    "    if i == 0:\n",
    "        ioloop.IOLoop.instance().stop()\n",
    "\n",
    "http_client = httpclient.AsyncHTTPClient()\n",
    "for url in open('urls.txt'):\n",
    "    i += 1\n",
    "    http_client.fetch(url.strip(), handle_request, method='HEAD')\n",
    "ioloop.IOLoop.instance().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate uuid and time generation\n",
    "import uuid\n",
    "import timeit\n",
    "timer=timeit.Timer('uuid.uuid1()','import uuid')\n",
    "timer.repeat(3, 300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Curl process \n",
    "\n",
    "import subprocess\n",
    "params = \"name=\"+name+\"&tags=\"+tags+\"&description=\"+description+\"&status=\"+status\n",
    "proc = subprocess.Popen(['curl', '-X', 'POST', '-d', \n",
    "                              params, url], stdout=subprocess.PIPE)\n",
    "(out, err) = proc.communicate()\n",
    "print out,err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript \n",
    "var kernel = Jupyter.notebook.kernel; \n",
    "var command = [\"notebookPath = \",\n",
    "               \"'\", window.document.body.dataset.notebookPath, \"'\" ].join('')\n",
    "//alert(command)\n",
    "kernel.execute(command)\n",
    "var command = [\"notebookName = \",\n",
    "               \"'\", window.document.body.dataset.notebookName, \"'\" ].join('')\n",
    "//alert(command)\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "p = str(os.getcwd()) \n",
    "print p + \"/\" + notebookName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
