{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ERDDAP-ScienceBase Processing Tool (exploration phase)\n",
    "##### A simple protoype\n",
    "\n",
    "\n",
    "##### Author:\n",
    "Tristan Wellman\n",
    "<br>twellman@usgs.gov\n",
    "<br>Biogeogeographic Characterization Branch\n",
    "<br>Core Science Analytics, Synthesis, and Libraries\n",
    "<br>U.S. Geological Survey, Denver, Colorado\n",
    "\n",
    "\n",
    "##### Description:  \n",
    "This python code performs initial steps in searching, \n",
    "<br>processing, and quality controlling datasets retrieved \n",
    "<br>from an ERDDAP data server, which are populated in \n",
    "<br>ScienceBase as persistent web items.\n",
    "\n",
    "The basic idea here is to interact with ERDDAP in a simple way.\n",
    "<br>The code automates some of the tedium in retrieving data, error \n",
    "<br>checking, setting up and updating a ScienceBase item for \n",
    "<br>each dataset examined, and pushes the data to ScienceBase.\n",
    "\n",
    "##### Functions:\n",
    "    (a) search ERDDAP by category, protocol, keyword       \n",
    "    (b) request / process data using 'advanced search url'\n",
    "    (c) examine / QC metadata and other information  \n",
    "    (d) create / modify ScienceBase records  \n",
    "    (e) Search for duplicate records by title, filenames, and dataset ID  \n",
    "\n",
    "##### Code Status:\n",
    "    Beta 0.3 1/30/2017 \n",
    "    A work in progress, testing and refining\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Operational Information \n",
    "\n",
    "#####      Input Files:\n",
    "    Two local files are required for operation in the working directory:\n",
    "    \n",
    "    1) A JSON dictionary file of operation inputs (\"name_file_dict.json\"). \n",
    "       The included script (\"generate_label_dictionary.ipnb\") can accomodate\n",
    "       some edits and will generate this file in the working directory \n",
    "       (\"tempdir\" in \"name_file_dict.json\"). \n",
    "\n",
    "    2) A status file showing processing dates, contacts, processing uuid,  \n",
    "       ERDDAP advanced search url, filenames, and upload and download status.\n",
    "       This file can be re-used if the URL_Flag option in (1) is True. \n",
    "       Otherwise, a new status file will be created.\n",
    "\n",
    "#####   Operation Flags:\n",
    "     Flags are stored in the JSON dictionary file (\"name_file_dict.json\"):\n",
    "\n",
    "     1) Whether to use an existing status file\n",
    "        ['url_flag'] = True or False \n",
    "     2) Whether to overwrite or skip data file processing (proc):\n",
    "        ['purge_proc'] = True or False or 'skip'\n",
    "     3) Whether to overwrite or skip ScienceBase file processing (sb):\n",
    "        ['purge_sb'] = True or False or 'skip'\n",
    "\n",
    "#####   To Run:\n",
    "    a) Set status filename, main flags, and other information for the JSON dictionary, \n",
    "       run dictionary script to create file (i.e. run \"generate_label_dictionary.ipnb\"). \n",
    "       Or copy an existing file into the working directory and adjust inputs manually.\n",
    "    b) Prescribe the temporary working directory (tempdir), immediately below. \n",
    "    c) Save code/files as appropriate.\n",
    "    d) Run this code (all cells).\n",
    "    e) Note: users (['login_name'] in JSON dictionary file) will be required to login to \n",
    "       ScienceBase when items are being created or modified.\n",
    "    \n",
    "#####   Output:\n",
    "    a) Creates/updates a status file in the working directory (see explanation above).\n",
    "    b) Downloads data and metadata files to a folder, each named by dataset ID.\n",
    "    c) Creates a misc. information file (info_dict.json) for each dataset.\n",
    "    d) Creates/updates ScienceBase records.\n",
    "    e) Creates date-time stamped input files in folder \"Prov_files\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/NOAA/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  working directory (temporary), should match 'local directory' in status file if url_flag = True \n",
    "\n",
    "# tempdir = \"/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/erddap_MBON_test2/\"\n",
    "# tempdir = \"/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/PacIOOS/\"\n",
    "tempdir = \"/Users/twellman/Documents/BCB_data_projects/OBIS_usa_database/NOAA/\"\n",
    "tempdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modules loaded\n"
     ]
    }
   ],
   "source": [
    "# python modules to load\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import pysb\n",
    "import uuid\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import sys\n",
    "import json\n",
    "from os import path, makedirs, remove\n",
    "from shutil import copyfileobj\n",
    "from urllib import urlencode, quote \n",
    "from collections import OrderedDict\n",
    "from IPython.core.display import display\n",
    "from timeit import default_timer as timer\n",
    "from lxml import etree, objectify\n",
    "\n",
    "print('modules loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitions loaded\n"
     ]
    }
   ],
   "source": [
    "# *******************************************************\n",
    "#    Definitions:  status = draft, work in progress \n",
    "# *******************************************************\n",
    "\n",
    "# write xml to python dictionary using recursion\n",
    "def xml_to_dict(xml_str):\n",
    "    def xml_to_dict_recursion(xml_object):\n",
    "        dict_object = xml_object.__dict__\n",
    "        if not dict_object:  # if empty dict returned\n",
    "            return xml_object\n",
    "        for key, value in dict_object.items():\n",
    "            dict_object[key] = xml_to_dict_recursion(value)\n",
    "        return dict_object\n",
    "    xml_obj = objectify.fromstring(xml_str)\n",
    "    return {xml_obj.tag: xml_to_dict_recursion(xml_obj)}\n",
    "\n",
    "# Adapt to Python v2+ ascii conversion issues (apparently fixed in python 3) \n",
    "def removeNonAscii(df_ascii): \n",
    "    df_ascii = df_ascii.apply(\n",
    "        lambda x: ''.join([i if 32 < ord(i) < 126 else \" \" for i in x]))\n",
    "    return df_ascii\n",
    "\n",
    "# Basic stats on requested ERDDAP data table (super simple for now)\n",
    "def get_stats(df_o):\n",
    "    agencies, counts = np.unique(df_o.loc[df_o[\"Institution\"] != (\"???\" or \"\")][\"Institution\"],return_counts=True)    \n",
    "    agencies = np.append(agencies, \"Unknown\")\n",
    "    counts = np.append(counts, df_o.shape[0] - np.sum(counts))\n",
    "    stats = [df_o[\"griddap\"].count(),\n",
    "             np.count_nonzero(df_o[\"tabledap\"]!=\"\"),\n",
    "             np.count_nonzero(df_o[\"griddap\"]!=\"\"),\n",
    "             len(agencies)-1 ]\n",
    "    print( \"\\n***** Query Results *****\\n\\nTotal number of datasets: %d\\nNumber of tables: %d\\nNumber of grids: %d\\\n",
    "        \\nNumber of agency groups providing data: %d\\n\" % tuple(stats))\n",
    "    header =  [\"** Agency Groups ** \", \"** Dataset count **\"]\n",
    "    a = max(len(str(max(agencies))),len(header[0]))\n",
    "    c = max(len(str(max(counts))),len(header[0]))\n",
    "    f  = '\\t{0:<%d}\\t{1:<%d}' % (a, c)\n",
    "    print(f.format(header[0], header[1]))\n",
    "    for p in zip(agencies, counts):\n",
    "        print(f.format(p[0],p[1]))\n",
    "    return\n",
    "\n",
    "# Show table\n",
    "def viewtable(df_view, cw, mr):\n",
    "    pd.set_option('display.max_colwidth', cw)\n",
    "    df_view.describe() \n",
    "    sys.stdout.write(\"\\nDisplay limited to a maximmum of %d datasets:\\n\" % mr)\n",
    "    df_view.columns.name = \"Data Index\"\n",
    "    display(df_view.ix[0:mr-1]) # assume 0 start index\n",
    "    return\n",
    "\n",
    "# bulk data download options: all data, all tables, or all grids  - refine later **\n",
    "def bulkoption(df):\n",
    "    bulkdict = {}\n",
    "    url = []\n",
    "    opts = [\"D\", \"T\", \"G\", \"C\"]\n",
    "    task = [\"all datasets\", \"all tables\", \"all grids\", \"by category\"]\n",
    "    resource = [ \"info\" , \"tabledap\", \"griddap\", \"categorize\"]\n",
    "    \n",
    "    for r in resource:\n",
    "        url.append(''.join(df.loc[df['Resource'] == r]['URL']))      \n",
    "    combo = zip(resource, task, url)\n",
    "    \n",
    "    # combine search info into dictionary\n",
    "    bulk_opts = zip(opts, combo)\n",
    "    for o, t in bulk_opts:\n",
    "        bulkdict[o] = t\n",
    "    return bulkdict\n",
    "\n",
    "# Generate url based on category and/or word/phrase search, \n",
    "# Note: doesn't include value constraints - a bit clunky, refine later **\n",
    "def build_url(srch_word, baseurl, protocol, cat_table, cindx, subcategory):\n",
    "    adv_url_html =  \"{}{}\".format(baseurl, \"/search/advanced.html?\")\n",
    "    prot_dict = {}\n",
    "    prot_dict = {\"A\" : \"(any)\", \"G\" : \"griddap\",\n",
    "                 \"W\" : \"WMS\",\"T\" : \"tabledap\"}\n",
    "    val_dict = OrderedDict()\n",
    "    val_dict = {\"maxLat\": \"\", \"minLon\": \"\", \"maxLon\": \"\",\n",
    "                \"minLat\": \"\", \"minTime\": \"\", \"maxTime\": \"\"}\n",
    "    udict = OrderedDict()\n",
    "    udict[\"searchFor\"] = srch_word\n",
    "    udict[\"protocol\"] = prot_dict[protocol]\n",
    "    for c in cat_table.Categorize:\n",
    "        udict[c] = \"(any)\"\n",
    "    if subcategory !=\"\": \n",
    "        udict[cat_table.get_value(cindx,\"Categorize\", takeable=False)] = subcategory\n",
    "    url_dict = udict.copy()\n",
    "    url_dict.update(val_dict)\n",
    "    data = urlencode(url_dict)\n",
    "    gen_url_html =  \"{}{}\".format(adv_url_html, data)\n",
    "    quote(gen_url_html, safe='') \n",
    "    gen_url_json = gen_url_html\n",
    "    gen_url_json = gen_url_json.replace(\".html\",\".json\")  \n",
    "    return gen_url_html, gen_url_json\n",
    "\n",
    "# Attempt url request, report error if encountered \n",
    "def url_request(url,rtype):\n",
    "    try:\n",
    "        if rtype == 'json':\n",
    "            r = requests.get(url).json()\n",
    "#            r.raise_for_status()\n",
    "        else:\n",
    "            r = requests.get(url, stream=True)\n",
    "#            r.raise_for_status()\n",
    "    except ValueError as e:  \n",
    "        sys.stdout.write(\"\\n{}{}\\n\\n{}\".format('** Flag ** : ', e,\"Search criteria could be too restrictive\"))\n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        sys.stdout.write(\"\\n{}\".format(\"\\n** Flag ** : system timeout - retry later\")) \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        sys.stdout.write(\"\\n** Flag ** : http request error\") \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        sys.stdout.write(\"\\n** Flag ** : systematic exception error\\nSearch url may need revision\") \n",
    "        sys.stdout.write(\"Error: {}\\n{}\".format(e, r.status_code)) \n",
    "        err = 'failed'\n",
    "    else:\n",
    "        err = 'passed'\n",
    "    return r, err\n",
    "\n",
    "# evaluate whether file directory exists and read/create status file (status)\n",
    "def chk_files(directory, nf_dict, task):\n",
    "    if task =='initialize':\n",
    "        file_sb = \"{}{}\".format(directory, nf_dict['init']['sb_json'])\n",
    "        if path.isfile(file_sb) and nf_dict['init']['url_flag'] != False:\n",
    "            with open(file_sb) as json_data:\n",
    "                status = json.load(json_data)\n",
    "            if len(status[0]['general information']) == 10:\n",
    "                status[0]['general information'].append({u'processing uuid' : str(uuid.uuid1())}) \n",
    "            if 'json' in status[0]['general information'][5]['data search json']:\n",
    "                url_chk = True\n",
    "            else:\n",
    "                sys.stdout.write(\"\\nNote: adv. search url was not found in sb_json (download status) file\")\n",
    "                url_chk = False\n",
    "            if status[1]['local_directory'] != tempdir:\n",
    "                sys.stdout.write(\"{}{}\".format('\\nWarning - local directory does not match entry in status file', \n",
    "                    'overwriting status file with current local directory'))\n",
    "                status[1]['local_directory'] = tempdir\n",
    "        else:\n",
    "            sys.stdout.write(\"\\nNote: a new sb_json (status file) is being created\")\n",
    "            url_chk = False\n",
    "         \n",
    "    # generate new status file, if warranted\n",
    "        if url_chk == False:\n",
    "            date = datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S')\n",
    "            status = [{'general information': \"none\"},{'local_directory': directory},{'datasets (id)': {}}] \n",
    "            status[0]['general information'] = nf_dict[u'general information']\n",
    "            status[0]['general information'][3]['data base url'] = nf_dict['init']['baseurl'] \n",
    "            status[0]['general information'][1]['file created (date-time)'] = date\n",
    "            status[0]['general information'][10]['processing uuid'] = str(uuid.uuid1()) \n",
    "            status[1]['local_directory'] = tempdir\n",
    "            with open(file_sb, 'w') as fp:\n",
    "                json.dump(status, fp, indent=4) \n",
    "                                 \n",
    "        nf_dict['init']['url_flag'] = url_chk\n",
    "        return status\n",
    "    else:\n",
    "        try:\n",
    "            if not path.exists(directory):\n",
    "                makedirs(directory)\n",
    "        except:\n",
    "            return\n",
    "        \n",
    "#  Search, request, and ultimately retrieve data information\n",
    "def retrieve_data(nf_dict, SB_status):\n",
    "    \n",
    "    if nf_dict['init']['url_flag'] != True:\n",
    "\n",
    "        subdir   = '/index'  \n",
    "        url_o = \"\".join((nf_dict['init']['baseurl'], subdir, nf_dict['init']['infotype'] ))\n",
    "        sys.stdout.write(\"\\nHome URL: %s \\n\\n\" % (url_o.rsplit( \".\", 1 )[ 0 ] + \n",
    "            nf_dict['init']['linktype'])) \n",
    "        response, err = url_request(url_o,'json')\n",
    "        \n",
    "        if nf_dict['init']['dataform'] == 'dframe':\n",
    "            \n",
    "            pd.set_option('display.notebook_repr_html', True)\n",
    "            pd.set_option('display.max_colwidth', -1)\n",
    "            pd.set_option('display.max_rows', 500)\n",
    "            colwidth = 100 # maximum column width in final request table\n",
    "            maxrows = 200 # maximum datasets to show in final request table\n",
    "\n",
    "    #   Main navigation links of the ERDDAP server\n",
    "            df = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "            df['URL'] = df['URL'].str.replace('.json', nf_dict['init']['linktype'])\n",
    "            sys.stdout.write(df.to_string(index=False, justify='left') + '\\n')\n",
    "\n",
    "    #   All datasets served by the ERDDAP server\n",
    "            url = ''.join(df.loc[df['Resource'] == 'info']['URL'])\n",
    "            sys.stdout.write(\"\\nLink to all datasets with metadata: %s\\n\" % url) \n",
    "\n",
    "    #   All datasets served by the ERDDAP server\n",
    "            url = \"\".join((nf_dict['init']['baseurl'], \"/tabledap/allDatasets\", nf_dict['init']['linktype']))\n",
    "            sys.stdout.write(\"\\nLink to all tabes, includes searchable information: %s\\n\\n\" % url) \n",
    "\n",
    "    #   Adjust links,settings\n",
    "            pd.set_option('display.max_colwidth',250)\n",
    "            df['URL'] = df['URL'].str.replace(nf_dict['init']['linktype'],'.json')\n",
    "\n",
    "    #   Set up info for search types (option, task, url)\n",
    "            bulk_opts_dict = bulkoption(df)\n",
    "\n",
    "    # else use full json method NOT DEVELOPED, incomplete **\n",
    "        else: \n",
    "            sys.stdout.write(\"{}{}\".format('Invalid entry, defaulting to .json format', '\\n')) \n",
    "            pprint.pprint(response.keys())\n",
    "            pp = pprint.PrettyPrinter(indent=2)\n",
    "            sys.exit()\n",
    "    #       add other functions later \n",
    "\n",
    "    # develop request table\n",
    "        df_request, gen_url_html, gen_url_json = search_url(df, bulk_opts_dict, nf_dict['init']['baseurl'] )\n",
    "        SB_status[0]['general information'][4]['data search url']  =  gen_url_html\n",
    "        SB_status[0]['general information'][5]['data search json'] =  gen_url_json\n",
    "    else:\n",
    "        sys.stdout.write(\"\\n{}{}{}\\n\\n\".format('\\n*** Automated search using information file: ', \n",
    "                         nf_dict['init']['sb_json'],'  ***'))                \n",
    "        response, err  = url_request(SB_status[0]['general information'][5]['data search json'],'json')\n",
    "        if err == 'failed': \n",
    "            sys.stdout.write(\"Identified problem using previous search url (*.json)\\nExiting program\")\n",
    "            sys.exit()\n",
    "        else:\n",
    "            df_request = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "    return df_request\n",
    "    \n",
    "\n",
    "# Download files - data chunking option, uses content disposition, tries <=5 times then bails   \n",
    "def download_file(url, fpath, altname, chunk):  \n",
    "    attempts = 0\n",
    "    while attempts < 5:\n",
    "        try:\n",
    "            response, err = url_request(url,'file')                 \n",
    "        except:\n",
    "            attempts += 1\n",
    "            sys.stdout.write(\"\\n\\tRequest error - url: %s , on attempt: %d\" % (url, attempts))\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        sys.stdout.write(\"\\n\\tRequest failed\")\n",
    "        return\n",
    "    if response.status_code == 200: \n",
    "        try:\n",
    "            d = response.headers['content-disposition']\n",
    "        except:\n",
    "            d = []\n",
    "            sys.stdout.write(\"\\n\\tGeneric filename using disposition:\\n\\t %s\" % url)\n",
    "        if d != []:\n",
    "            locf = ''.join(re.findall(\"filename=(.+)\", d))\n",
    "        else:\n",
    "            locf = altname \n",
    "        path = \"{}/{}\".format(fpath, locf)\n",
    "    \n",
    "    # Stream file object, no data chunking\n",
    "        if chunk == \"OFF\":\n",
    "            with open(path, 'wb') as f:\n",
    "                response.raw.decode_content = True\n",
    "                copyfileobj(response.raw, f)\n",
    "                sys.stdout.write(\"\\n\\tDownloaded %s \" % locf)\n",
    "                response.close()\n",
    "                return path    \n",
    "    # Chunk data - adjust iter_content size (bytes) ** -- not tested\n",
    "        else:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "                sys.stdout.write(\"\\n\\tDownloaded %s \" % locf)\n",
    "                response.close()\n",
    "                return path\n",
    "    else:\n",
    "        sys.stdout.write(\"\\n\\t%s returned request error status code: %s\" % (altname, response.status_code)) \n",
    "        response.close()\n",
    "        return \"None\"\n",
    "    sys.stdout.write(\"\\nRequest error status code: %s\" % r.status_code)\n",
    "    \n",
    "     \n",
    "# Attempt retrieval of data files, metadata, and misc information\n",
    "# check information existence, purge (i.e. update) data files if commanded\n",
    "def data_proc(status, df_request, nf_dict):\n",
    "    \n",
    "    if nf_dict['init']['purge_proc'] == 'skip':\n",
    "        sys.stdout.write(\"\\n\\t - passed over data processing step - as directed\\n\")\n",
    "        return \n",
    "            \n",
    "    # loop through datasets\n",
    "    for index, row in df_request.iterrows():\n",
    "        \n",
    "    # ensure existence of working directory (temporary?) \n",
    "        fpath = \"{}{}\".format(status[1]['local_directory'], row[\"Dataset ID\"])\n",
    "        chk_files(fpath, {}, '--')\n",
    "        \n",
    "    # if warranted create or modify dataset download record (ERDDAP dataset ID) \n",
    "        if row['Dataset ID'] not in status[2]['datasets (id)']:\n",
    "            status[2]['datasets (id)'][row['Dataset ID']] = OrderedDict([('sb_id',\"None\"), ('dataset_url', \"None\"), \n",
    "                    ('download','incomplete'), ('upload','incomplete'), ('files', OrderedDict())])\n",
    "            status[2]['datasets (id)'][row['Dataset ID']]['files'] = { k: \"None\" for k in (\n",
    "                    nf_dict['init']['dataproc'] + nf_dict['init']['metafiles'] + ['info_dict','info_request'])}  \n",
    "  \n",
    "    # find ERDDAP dataset type (table, grid, wms), assumes type entries are unique \n",
    "        url_base = ''.join([row[\"tabledap\"], row[\"griddap\"], row[\"wms\"]])\n",
    "        status[2]['datasets (id)'][row['Dataset ID']]['dataset_url'] = url_base + \".html\"\n",
    "\n",
    "    # retrieve requested record information \n",
    "        if status[2]['datasets (id)'][row['Dataset ID']]['download'] != 'incomplete':\n",
    "            sys.stdout.write(\"\\n\\n** Dataset ** : %s\\n\\n  files downloaded:\" % row['Dataset ID'])\n",
    "        else:\n",
    "            sys.stdout.write(\"\\n** Processing Dataset ** : %s - %s\\n\\n**  Retrieving datafiles:\"\n",
    "                % (row[\"Dataset ID\"], row[\"Title\"]))\n",
    "\n",
    "        # retrieve requested ERDDAP datasets\n",
    "            for d in nf_dict['init']['dataproc']:\n",
    "                if status[2]['datasets (id)'][row['Dataset ID']]['files'][d] == \"None\":\n",
    "                    url = ''.join(\"{}{}\".format(url_base, d))\n",
    "                    altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d))\n",
    "                    f = download_file(url,fpath, altname, \"OFF\") \n",
    "                    status[2]['datasets (id)'][row['Dataset ID']]['files'][d] = f\n",
    "            sys.stdout.write(\"\\n\\tStep completed...\\n\")\n",
    "            \n",
    "            sys.stdout.write(\"\\n    Processing metafiles:\")\n",
    "            for d in nf_dict['init']['metafiles']:\n",
    "                if status[2]['datasets (id)'][row['Dataset ID']]['files'][d] == \"None\":\n",
    "                    url = ''.join(\"{}{}\".format(url_base, d))\n",
    "                    altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d)) \n",
    "                    f = download_file(url,fpath, altname, \"OFF\") \n",
    "                    status[2]['datasets (id)'][row['Dataset ID']]['files'][d] = f\n",
    "            sys.stdout.write(\"\\n\\tStep completed...\\n\")\n",
    "            \n",
    "            with open(status[1]['local_directory'] + nf_dict['init']['sb_json'], 'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "        # process data table information \n",
    "            sys.stdout.write(\"\\n    Processing table information:\")\n",
    "\n",
    "        # info dictionary \n",
    "            info_request = {}\n",
    "            info_dict = {'summary_info': {},'data_info' : {} }\n",
    "            \n",
    "        # retrieve other ERDDAP information as dictionary (info_request, RSS, or basic xml, etc.)\n",
    "            if status[2]['datasets (id)'][row['Dataset ID']]['files']['info_request'] == \"None\":\n",
    "                for d in nf_dict['init']['table_info']:\n",
    "                    entry = row[d]\n",
    "                    \n",
    "                # store info depending on the type, a work in progress\n",
    "                    if \"http\" in entry and d != 'Summary':\n",
    "                        ext = entry.rsplit(\".\",1)[1]\n",
    "                        sys.stdout.write(\"\\n\\tRetrieving additional url to request: %s \" % d)\n",
    "                        altname = ''.join(\"{}{}\".format(row[\"Dataset ID\"], d))\n",
    "                        \n",
    "                    # info file\n",
    "                        if d == \"Info\":\n",
    "                            info_request, err = url_request(entry,'json')\n",
    "                            if err != 'error':\n",
    "                                f = \"{}{}\".format(fpath,\"/info_request.json\")\n",
    "                                with open(f, 'w') as fp:\n",
    "                                    json.dump(info_request, fp, indent=4)\n",
    "                                status[2]['datasets (id)'][row['Dataset ID']]['files']['info_request'] = f\n",
    "                        \n",
    "                    # parse rss file\n",
    "                        elif d == 'RSS': \n",
    "                            response, err = url_request(entry,'file')  \n",
    "                            if response.status_code == 200:\n",
    "                                response.raw.decode_content = True\n",
    "                                tree = etree.parse(response.raw)\n",
    "                                root = tree.getroot()\n",
    "                                label = root.tag.rsplit(\"}\",1)[1]\n",
    "                                ns = {label: root.nsmap[None]}\n",
    "                                f = \"{}{}{}{}\".format(\"//\",label,\":\", 'item/*')\n",
    "                                modinfo = tree.xpath(f, namespaces=ns)\n",
    "                                info_dict['summary_info']['rss'] = {}\n",
    "                                for r in modinfo:\n",
    "                                    info_dict['summary_info']['rss'][r.tag.rsplit(\"}\",1)[1]] = r.text \n",
    "                                f= '//rss:pubDate'\n",
    "                                modinfo = tree.xpath(f, namespaces=ns)\n",
    "                                for r in modinfo:\n",
    "                                    info_dict['summary_info']['rss']['pubDate'] = r.text\n",
    "                            response.close()\n",
    "                        \n",
    "                    # convert general xml request to dictionary\n",
    "                        elif ext == \"xml\":  \n",
    "                            response, err = url_request(entry,'file')  \n",
    "                            if response.status_code == 200:\n",
    "                                response.raw.decode_content = True\n",
    "                                tree = etree.parse(response.raw)\n",
    "                                xml_string = etree.tostring(tree)\n",
    "                                info_dict['data_info'][d] = xml_to_dict(xml_string)\n",
    "                            response.close()\n",
    "                        \n",
    "                    # dump misc content as text\n",
    "                        else:\n",
    "                            sys.stdout.write('file type: %s is not detailed.'\n",
    "                                             '\\n\\tBulk printing response as string' % ext ) \n",
    "                            info_dict['data_info'][d] = entry\n",
    "                    \n",
    "                # store other in general section ('summary_info')\n",
    "                    else:\n",
    "                        info_dict['summary_info'][d] = entry\n",
    "            \n",
    "            # update file information\n",
    "                f = \"{}{}\".format(fpath, \"/info_dict.json\")\n",
    "                with open(f, 'w') as fp:\n",
    "                    json.dump(info_dict, fp, indent=4)\n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['files']['info_dict'] = f\n",
    "            \n",
    "        # check if downloads complete\n",
    "            bstat = status[2]['datasets (id)'][row['Dataset ID']]['files']\n",
    "            if \"None\" not in bstat.itervalues():   \n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['download'] = \"YES\"\n",
    "            else:\n",
    "                status[2]['datasets (id)'][row['Dataset ID']]['download'] = \"incomplete\"\n",
    "            with open(status[1]['local_directory'] + nf_dict['init']['sb_json'], 'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "        sys.stdout.write(\"\\n\\tStep completed...\\n\")         \n",
    "        \n",
    "    # work with only one dataset = for testing purposes Tristan      \n",
    "    #    break\n",
    "    return\n",
    "\n",
    "# Search ERDDAP datasets using a simple interface (otherwise use adv url and bypass)  - incomplete ** \n",
    "def search_url(df, bulk_opts_dict, baseurl):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        print(\"Request ERDDAP datasets using a retrieval option <letter>: \\n\\t%s\\n\\t%s\" \n",
    "                  % (\"(B) Bulk type constraint, or\", \"(S) Search (categorical, keyword(s), phrase)\"))\n",
    "        entry = (raw_input()).upper()\n",
    "        \n",
    "    # bulk file read \n",
    "        if entry == \"B\":  \n",
    "            while True:\n",
    "                sys.stdout.write(\"\\nRequest all: (D) datasets, (T) Tables, or (G) Grids\\n\")\n",
    "                bulk = (raw_input()).upper()\n",
    "                if bulk in bulk_opts_dict:\n",
    "                    link = ''.join(bulk_opts_dict[bulk][2]).replace(\".json\",\".html\")\n",
    "                    print \"\\nBulk Request %s\\nURL: %s\" % ( bulk_opts_dict[bulk][1], link )\n",
    "                    response, err = url_request(bulk_opts_dict[bulk][2],'json')\n",
    "                    df_query = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                    get_stats(df_query)\n",
    "                    return df_query, link, bulk_opts_dict[bulk][2] \n",
    "                else:\n",
    "                    print (\"\\nInvalid command - enter an indicated letter option (*)\") \n",
    "    \n",
    "    # Search by category, search word(s), and/or protocol (data type) \n",
    "        elif entry == \"S\":  \n",
    "            sys.stdout.write(\"\\n*** Note: custom search methods are loosely fitted - in progress ***\")\n",
    "            response, err = url_request(bulk_opts_dict[\"C\"][2],'json')\n",
    "            df_cquery = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "            df_cquery.columns.name = \"Search Index\"\n",
    "            display(df_cquery)\n",
    "            \n",
    "        # Category search\n",
    "            sys.stdout.write(\"Select search index for category [left column] (optional: # is not index --> skip):\\n\")\n",
    "            try:\n",
    "                cindx = int(raw_input())\n",
    "            except:\n",
    "                cindx = [] \n",
    "            squery_entry = \"\"\n",
    "            dfl = list(df_cquery.index.values)\n",
    "            if cindx in dfl:\n",
    "                url = ''.join(df_cquery.iloc[[cindx]][\"URL\"])\n",
    "                response, err = url_request(url,'json')\n",
    "                df_squery = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                df_squery.columns.name = \"Search Index\"\n",
    "                display(df_squery)\n",
    "                sindx = raw_input(\"Select search index for subcategory [left column] (optional: # is not index --> skip):\\n\")\n",
    "                try:\n",
    "                    sindx = int(sindx)\n",
    "                except:\n",
    "                    sindx = []\n",
    "                dfl = list(df_squery.index.values)\n",
    "                if sindx in dfl:\n",
    "                    squery_entry = df_squery.get_value(sindx,\"Category\", takeable=False)\n",
    "                    url = ''.join(df_squery.iloc[[sindx]][\"URL\"])\n",
    "                    response, err = url_request(url,'json')\n",
    "                    df_query = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])  \n",
    "                else:\n",
    "                    sys.stdout.write(\"\\n\\tEntered search index was not in list, skipped subcategory search.\\n\\n\")       \n",
    "            else:\n",
    "                sys.stdout.write(\"\\n\\tEntered search index was not in list, skipped category refinement.\\n\\n\")   \n",
    "\n",
    "        # Word or phrase search \n",
    "            sys.stdout.write(\"\\nOptional keyword or phrase search\\nEnter space-delimited search word(s) or qouted phrase (blank --> skip):\\n\")\n",
    "            search = raw_input()\n",
    "            if search == '': \n",
    "                sys.stdout.write(\"Search input was blank, skipped word search\")  \n",
    "            \n",
    "        # Protocol (data type) search constraint - currently does not examine subsets\n",
    "            sys.stdout.write(\"\\nDefine allowed protocol (data type): \\n\\t%s\\n\\t%s\\n\\t%s\\n\\t%s\\n\\t%s\\n\" \n",
    "                  % (\"(A) All data types,\", \"(G) Griddap, or\", \"(T) Tabledap, or\",\"(W) Wms\",\"(default --> All types)\"))\n",
    "            protocol = (raw_input()).upper()\n",
    "            if protocol not in [\"A\", \"G\", \"T\", \"W\"]:\n",
    "                protocol = \"A\"\n",
    "            \n",
    "        # Gather pertinent info, build custom url for http services\n",
    "            comb = ''.join([search, protocol, squery_entry])\n",
    "            if comb != \"A\":\n",
    "                gen_url_html, gen_url_json = build_url(search, baseurl, protocol, df_cquery, cindx, squery_entry)   \n",
    "                sys.stdout.write(\"\\n'Advanced' search url:\\n%s\\n\" % gen_url_html)\n",
    "                try:\n",
    "                    url_request(gen_url_json,'json')\n",
    "                except:\n",
    "                    sys.stdout.write(\"Identified exception during url request\")\n",
    "                    return\n",
    "                df_request = pd.DataFrame(response['table']['rows'], columns=response['table']['columnNames'])\n",
    "                get_stats(df_request)        \n",
    "                return df_request, gen_url_html, gen_url_json\n",
    "            else:\n",
    "                sys.stdout.write(\"\\nNo search constraints were detected - retry\\n\\n\")  \n",
    "        else:     \n",
    "            print (\"\\nInvalid command - enter a letter option (*)\\n\") \n",
    "\n",
    "# set permissions\n",
    "def set_permissions(item_id, acls):\n",
    "    sb_base_url = \"https://www.sciencebase.gov/catalog/item/\" + item_id + \"/permissions/\"\n",
    "    return sb._get_json(sb._session.put( sb_base_url, data=json.dumps(acls)))\n",
    "\n",
    "# adjust read/write (task) privleges \n",
    "def set_acls(acls, names, task):\n",
    "    if 'inheritsFromId' in acls[task]:\n",
    "        del acls[task]['inheritsFromId']\n",
    "    acls[task]['inherited'] = False\n",
    "    new_acls = []\n",
    "    for p in names:\n",
    "        new_acls.append(p)\n",
    "    acls[task]['acl'] = new_acls\n",
    "    return acls\n",
    "\n",
    "# reformat date entries\n",
    "def date_reform(date_input):\n",
    "    if date_input:\n",
    "        date_rec = str(parser.parse(date_input, ignoretz=True)).split(\" \",1)[0]\n",
    "        if date_rec == datetime.date.today():\n",
    "            date_rec = \"{}{}\".format(date_input,'_recheck') \n",
    "        return date_rec\n",
    "    else:\n",
    "        return \"\"\n",
    "        \n",
    "# pull info from ERDDAP info table as pandas dataframe (adhoc corrections for now, work in progress)\n",
    "def populate_sbase(attribute , rmv, info_frame):\n",
    "    indx = info_frame[info_frame['Attribute Name'] == attribute].index.tolist()\n",
    "    if indx:\n",
    "        entry = info_frame.get_value(indx[0],\"Value\", takeable=False)\n",
    "        if rmv == 'yes':\n",
    "            if isinstance(entry, basestring):\n",
    "                entry = re.sub(r'\\,(?! )', ', ', re.sub(r' +', ' ', entry)) # ensure space after commas\n",
    "#            entry = re.sub(r'\\.(?! )', '. ', re.sub(r' +', ' ', entry)) # ensure space after period\n",
    "                entry = entry.rstrip() # remove carriage returns /n  \n",
    "                entry = entry.replace('?s',\"'s\") # remove erroneous \"?\" unicode trans errors\n",
    "        return entry\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "# process contact information from erddap information file\n",
    "def proc_erddap_contacts(base_key, info_file, type_label, info_keys, label_keys):     \n",
    "    contact_dict = {}\n",
    "    for i, k in enumerate(info_keys):\n",
    "        srch_key = (base_key + '_' + k)\n",
    "        if info_file['Attribute Name'].str.contains(srch_key).any():\n",
    "            contact_dict[label_keys[i]] = populate_sbase(srch_key,'yes', info_file).decode('utf-8')\n",
    "        else:\n",
    "            contact_dict[label_keys[i]] = \"\"\n",
    "    contact_dict[u'type'] = type_label\n",
    "    if u'organization' in contact_dict:\n",
    "        cpy = contact_dict[u'organization']\n",
    "        contact_dict[u'organization'] = {u'displayText': cpy}\n",
    "    \n",
    "    return contact_dict\n",
    "\n",
    "# purge function to restart data and metadata processing, if True \n",
    "# i.e. retrieved files will be re-processed and will overwrite existing files \n",
    "def purge_option(SB_status, nf_dict):\n",
    "    if nf_dict['init']['purge_proc'] == True:\n",
    "        sys.stdout.write(\"\\n** Request to reset (purge) data/metadata status indicators\")\n",
    "        dataproc = nf_dict['init']['dataproc']\n",
    "        metafiles = nf_dict['init']['metafiles']\n",
    "        for dataset in SB_status[2]['datasets (id)']:\n",
    "            SB_status[2]['datasets (id)'][dataset]['download'] = 'incomplete'\n",
    "            SB_status[2]['datasets (id)'][dataset]['upload'] = 'incomplete'\n",
    "            SB_status[2]['datasets (id)'][dataset]['files'] = { k: \"None\" for k in \n",
    "                (dataproc + metafiles + ['info_dict','info_request']) }\n",
    "        with open(SB_status[1]['local_directory'] + nf_dict['init']['sb_json'], 'w') as fp:\n",
    "            json.dump(SB_status, fp, indent=4)\n",
    "        sys.stdout.write(\"\\n\\t - Reset (purge) complete\")\n",
    "    elif nf_dict['init']['purge_proc'] == False:\n",
    "        sys.stdout.write(\"\\n** Request to append incomplete file information to existing status file\\n\")\n",
    "               \n",
    "# Create and update SB records, modify as needed\n",
    "def SciBase_item(status, nf_dict):\n",
    "    \n",
    "    if nf_dict['init']['purge_sb'] == 'skip':\n",
    "        sys.stdout.write(\"\\n\\t - passed over ScienceBase processing step - as directed\\n\")\n",
    "        return\n",
    "    else:\n",
    "        sys.stdout.write(\"{}\".format('\\n\\n\\tLogin to ScienceBase - '))\n",
    "        sb = pysb.SbSession()\n",
    "        sb.loginc(str(nf_dict['init']['login_name']))\n",
    "        time.sleep(5)\n",
    "    \n",
    "    for dataset in status[2]['datasets (id)']:\n",
    "        \n",
    "        data = status[2]['datasets (id)'][dataset]\n",
    "        \n",
    "        if data[\"download\"] != 'incomplete':\n",
    "\n",
    "        # Open erddap information files (info_request and info_dict, as json)\n",
    "            with open(data['files']['info_request']) as fp:    \n",
    "                info_request = json.load(fp)\n",
    "            info_frame = pd.DataFrame(info_request['table']['rows'], columns=info_request['table']['columnNames'])\n",
    "            with open(data['files']['info_dict']) as fp:    \n",
    "                info_dict = json.load(fp)\n",
    "            sys.stdout.write(\"\\n   Processing dataset: %s into ScienceBase\\n\" % dataset)\n",
    "\n",
    "        # check SB item existence (visibility)\n",
    "            chk = True\n",
    "            if data[\"sb_id\"] != \"None\":\n",
    "                try:\n",
    "                    SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "                except:\n",
    "                    sys.stdout.write(\"\\n{}{}{}\".format('Warning: SB item ', data[\"sb_id\"],' was NOT located - creating new item'))\n",
    "                    chk = False\n",
    "                else:\n",
    "                    sys.stdout.write(\"\\n\\t{}{}{}\".format('SB item: ', data[\"sb_id\"],' was found in ScienceBase'))\n",
    "            else:\n",
    "                chk = False\n",
    "            \n",
    "        # create new sciencebase item, if an existing item is not found \n",
    "            if chk == False:\n",
    "                new_item_info = {'title': populate_sbase(\"title\", 'yes', info_frame),\n",
    "                    'parentId': sb.get_my_items_id(),\n",
    "                    'provenance': {'annotation': nf_dict['prov_state']}}\n",
    "                SB_rec = sb.create_item(new_item_info)\n",
    "                data[\"sb_id\"] = SB_rec['id']\n",
    "                status[2]['datasets (id)'][dataset][\"sb_id\"] = SB_rec['id']\n",
    "                sys.stdout.write(\"\\n\\t{}{}\".format('Created new ScienceBase item:', data[\"sb_id\"]))\n",
    "            \n",
    "        # check SB item permissions, if no permissions create new item\n",
    "            if u'USER:' + nf_dict['init']['login_name'] in SB_rec['permissions'][u'write'][u'acl']:\n",
    "                sys.stdout.write(\"\\n\\t** Verified ** user has write privileges for item\")\n",
    "            else:\n",
    "                sys.stdout.write(\"\\n *****  WARNING  ***** : user is without write privileges - creating new item\")\n",
    "                data[\"sb_id\"] = \"None\"\n",
    "                new_item_info = {'title': populate_sbase(\"title\",'yes', info_frame),\n",
    "                    'parentId': sb.get_my_items_id(),\n",
    "                    'provenance': {'annotation': nf_dict['prov_state']}}\n",
    "                SB_rec = sb.create_item(new_item_info)\n",
    "\n",
    "            sys.stdout.write(\"\\n\\tChecking files, uploading/updating files and metadata\")\n",
    "            \n",
    "        # gather list of files currently in item's 'files' and 'facets'\n",
    "            uploaded_files = []\n",
    "            if 'files' in SB_rec:\n",
    "                uploaded_files = [f_ex['name'] for f_ex in SB_rec['files']]  \n",
    "            if 'facets' in SB_rec:\n",
    "                for facet in SB_rec['facets']:\n",
    "                    if 'files' in facet:\n",
    "                        for f in facet['files']:\n",
    "                            uploaded_files.append(f['name'])\n",
    "                        \n",
    "        # upload only new files \n",
    "            if nf_dict['init']['purge_sb'] == False:\n",
    "                for f in (nf_dict['init']['metafiles'] + nf_dict['init']['dataproc']):\n",
    "                    fn = data['files'][f].rsplit( \"/\", 1 )[ 1 ] \n",
    "                    if fn not in uploaded_files:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"uploading new file to item\"))\n",
    "                        sb.uploadFileToItem(SB_rec, data['files'][f])  \n",
    "        # else upload/update all files\n",
    "            elif nf_dict['init']['purge_sb'] == True:\n",
    "                for f in (nf_dict['init']['metafiles'] + nf_dict['init']['dataproc']):\n",
    "                    fn = data['files'][f].rsplit( \"/\", 1 )[ 1 ] \n",
    "                    if fn not in uploaded_files:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"uploading new file to item\"))\n",
    "                        sb.uploadFileToItem(SB_rec, data['files'][f])\n",
    "                    else:\n",
    "                        sys.stdout.write(\"\\n\\t{}\".format(\"updating existing file in item\"))\n",
    "                        sb.replace_file(data['files'][f], SB_rec)\n",
    "                        SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "                   \n",
    "            sys.stdout.write(\"\\n\\tModifying ScienceBase record information\")        \n",
    "                    \n",
    "        # retrieve latest SB item after file uploads/updates \n",
    "            SB_rec = sb.get_item(data[\"sb_id\"])\n",
    "            \n",
    "        # sb date tags, create if needed. \n",
    "            rep = status[0]['general information'][2]['file modified (date-time)']\n",
    "            if rep == 'none' or 'dates' not in SB_rec:                   \n",
    "                SB_rec[u'dates']  = [{u\"type\":u\"Item submission\",\n",
    "                    u\"dateString\":str(datetime.date.today()), u\"label\": u\"Item completed\"}]       \n",
    "            elif SB_rec[u'dates'][0][u\"label\"] == u\"Item submission\":\n",
    "                SB_rec[u'dates'] = [SB_rec[u'dates'][0]]\n",
    "            else:\n",
    "                SB_rec[u'dates'] = []\n",
    "                \n",
    "        # add in erddap record dates, if present   \n",
    "            for i, k in enumerate(nf_dict['date_keys'] ):\n",
    "                entry = date_reform(populate_sbase(k,'no', info_frame))\n",
    "                if entry:\n",
    "                    SB_rec[u'dates'].append({\n",
    "                        u\"type\": nf_dict['date_labels'][i],\n",
    "                        u\"dateString\":entry,\n",
    "                        u\"label\": nf_dict['date_labels'][i] })\n",
    "                    \n",
    "        # add in file retrieval (download) date using OS call \n",
    "            if nf_dict['init']['metafiles'][0]:\n",
    "                fn = nf_dict['init']['metafiles'][0]\n",
    "                date_input = path.getmtime(data['files'][fn])\n",
    "            else:\n",
    "                fn = nf_dict['init']['dataproc'][0]\n",
    "                date_input = path.getmtime(data['files'][fn])  \n",
    "            Int2date = datetime.datetime.fromtimestamp(date_input)\n",
    "            retrieve = datetime.datetime.strftime(Int2date, '%Y-%m-%d')\n",
    "            SB_rec[u'dates'].append( {u\"type\":u\"Retrieved from source\",\n",
    "                u\"dateString\":retrieve.decode('utf-8'),\n",
    "                u\"label\":u\"Retrieved from source\"} )\n",
    "                                 \n",
    "        # update citation information, if available, else remove tag\n",
    "            study_cite = populate_sbase('associatedReferences','yes', info_frame)\n",
    "            data_cite = populate_sbase('bibliographicCitation','yes', info_frame)\n",
    "            if data_cite:\n",
    "                SB_rec['citation'] = \"{}{}\".format(\"Data citation - \", data_cite)\n",
    "                if study_cite:\n",
    "                    SB_rec['citation'] =  \"{}{}\".format(SB_rec['citation'],  \n",
    "                        \"{}{}\".format(\". Study citation - \", study_cite))\n",
    "            elif study_cite:\n",
    "                SB_rec['citation'] = \"{}{}\".format(\"Study citation - \", study_cite)\n",
    "            elif 'citation' in SB_rec:\n",
    "                SB_rec['citation'] = \"\"\n",
    "                \n",
    "        # update provenance tag\n",
    "            SB_rec[u'provenance']['annotation'] = \"{}{}{}\".format(nf_dict['prov_state'],' DataID: ', dataset)\n",
    "   \n",
    "        # update title\n",
    "            SB_rec[\"title\"] = populate_sbase(\"title\",'yes', info_frame)\n",
    "\n",
    "        # retrieve license information\n",
    "            entry_license = populate_sbase(\"license\",'yes', info_frame)\n",
    "\n",
    "        # update summary + add license information\n",
    "            SB_rec[\"body\"] = \"{}{}{}\".format(populate_sbase(\"summary\",'yes', info_frame),'&nbsp; &nbsp;\\n<br> \\n<br>',entry_license)\n",
    "\n",
    "        # provide OPeNDAP weblinks to source\n",
    "            webrec = []\n",
    "            for w in nf_dict['init']['webnames']:\n",
    "                d_url = status[2]['datasets (id)'][dataset][\"dataset_url\"].replace('.html', w)\n",
    "                rec = {\n",
    "                    u'hidden': False,\n",
    "                    u'rel': u'related',\n",
    "                    u'title': nf_dict['file_dict'][w],\n",
    "                    u'type': u'OPeNDAP weblinks - data provenance trace',\n",
    "                    u'uri': d_url} \n",
    "                webrec.append(rec)\n",
    "            SB_rec['webLinks'] = webrec \n",
    "            \n",
    "        # retrieve contact information in erddap info record\n",
    "            url_b = \"{}{}{}{}\".format(nf_dict['init']['baseurl'],'/info/', dataset,'/index.json')\n",
    "            info_req = requests.get(url_b).json()\n",
    "            info_file = pd.DataFrame(info_req['table']['rows'], columns=info_req['table']['columnNames'])     \n",
    "            \n",
    "        # search for contacts, \n",
    "        # if none, allow autopopulated metadata contacts, if present\n",
    "            store_name = []\n",
    "            for indx,tag in enumerate(nf_dict['erddap_contype']):\n",
    "                label = \"{}{}\".format(tag,'_name')\n",
    "                if info_file['Attribute Name'].str.contains(label).any():\n",
    "                     store_name.append(proc_erddap_contacts(tag, info_file, nf_dict['sb_contype'][indx].decode('utf-8'),\n",
    "                        nf_dict['info_keys'], nf_dict['label_keys']))\n",
    "            if not store_name:\n",
    "                store_name = SB_rec['contacts']     \n",
    "            store_name.append(nf_dict['BCB_contact'])\n",
    "l\n",
    "\n",
    "        # eliminate duplicate contacts\n",
    "            seen = set()\n",
    "            SB_rec['contacts'] = [item for item in store_name \n",
    "                                  if item['name'] not in seen and not seen.add(item['name'])]\n",
    "        # set permissions \n",
    "            acls = SB_rec['permissions']\n",
    "            set_acls(acls, nf_dict['read_names'],'read')\n",
    "            set_acls(acls, nf_dict['write_names'],'write')\n",
    "            #   set_permissions(SB_rec['id'], acls) - currently not used (sub 2 lines below)\n",
    "            sb_base_url = \"https://www.sciencebase.gov/catalog/item/\" + SB_rec['id'] + \"/permissions/\"\n",
    "            sb._get_json(sb._session.put( sb_base_url, data=json.dumps(acls)))\n",
    "\n",
    "        # remove redundancy in tag names     \n",
    "            seen = set()\n",
    "            SB_rec[u'tags'] = [item for item in SB_rec[u'tags'] \n",
    "                                  if item['name'] not in seen and not seen.add(item['name'])]\n",
    "                \n",
    "        # check for potential duplicate items in sciencebase (title, filename(s), dataset id)\n",
    "            sys.stdout.write(\"\\n\\t{}{}\".format('Searching for duplicate', \n",
    "                ' sb items using filenames, item title, and dataset ID'))  \n",
    "            blk_search = sb.find_items_by_title(SB_rec[\"title\"])[u'items']\n",
    "            if 'files' in SB_rec:\n",
    "                for f in SB_rec['files']:\n",
    "                    blk_search.extend(sb.find_items_by_any_text(f['name'])[u'items'])        \n",
    "            blk_search.extend(sb.find_items_by_any_text(dataset)[u'items'])  \n",
    "            seen = set()\n",
    "            search_id = [item['id'] for item in blk_search \n",
    "                        if item['id'] not in seen and not seen.add(item['id'])]\n",
    "            search_id = filter(lambda a: a != data[\"sb_id\"], search_id )            \n",
    "            if search_id:\n",
    "                sys.stdout.write(\"\\n\\n\\t\\t{}\".format('*** WARNING possible duplicate item(s) ***'))\n",
    "                status[2]['datasets (id)'][dataset]['Possible replicate items'] = search_id\n",
    "                for s in search_id:\n",
    "                    sys.stdout.write(\"\\n\\t\\t\\t{}\".format(s))\n",
    "            else:\n",
    "                sys.stdout.write(\"\\n\\t\\t{}\".format('pass: item duplicates not identified'))\n",
    "\n",
    "        # save updates, file\n",
    "            sys.stdout.write(\"\\n\\tScienceBase processing completed\\n\")\n",
    "            sb.updateSbItem(SB_rec)\n",
    "            status[2]['datasets (id)'][dataset][\"upload\"] = \"YES\"\n",
    "            status[2]['datasets (id)'][dataset][\"sb_id\"] = data[\"sb_id\"]\n",
    "            with open(status[1]['local_directory'] + nf_dict['init']['sb_json'],'w') as fp:\n",
    "                json.dump(status, fp, indent=4)\n",
    "                \n",
    "        # testing - work with only one dataset = for testing purposes (break) - Tristan       \n",
    "        #    break \n",
    "        else:\n",
    "            sys.stdout.write(\"\\nInfo for dataset %s - incomplete not processed\\n\" % dataset)  \n",
    "\n",
    "print('definitions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search ERDDAP data repository, develop search criteria\n",
      "\n",
      "\n",
      "*** Automated search using information file: file_sb.json  ***\n",
      "\n",
      "\n",
      "** Request to append incomplete file information to existing status file\n",
      "\n",
      "\n",
      "** Processing data and metadata files\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfmSFNH\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfm1Q\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfm5Q\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfmBT\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfmFT\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdCinpKfmRPC\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : prboSefiDiet - Farallon Island Seabird Diet Summary\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tprboSefiDiet.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : prboSefiPhen - Farallon Island Seabird Phenology Summary\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tprboSefiPhen.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : prboSefiPop - Farallon Island Seabird Population Summary\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tprboSefiPop.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : prboSefiProd - Farallon Island Seabird Productivity Summary\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tprboSefiProd.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdGlobecMoc1\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdGlobecBirds\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : erdGlobecVpt\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "\n",
      "** Dataset ** : wocecpr\n",
      "\n",
      "  files downloaded:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : nwioosHudFishDetails - NWFSC HUD Fish Species Details\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tnwioosHudFishDetails.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : nwioosHudFishOccur - NWFSC HUD Fish Species Occurrence\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tnwioosHudFishOccur.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : nwioosHudPredPrey - NWFSC HUD Predators and Prey\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tnwioosHudPredPrey.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : nwioosHudRef - NWFSC HUD References\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tnwioosHudRef.iso19115 returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing Dataset ** : aadcArgos - OBIS - ARGOS Satellite Tracking of Animals\n",
      "\n",
      "**  Retrieving datafiles:\n",
      "\taadcArgos.nc returned request error status code: 500\n",
      "\taadcArgos.ncHeader returned request error status code: 500\n",
      "\taadcArgos.csv returned request error status code: 500\n",
      "\tStep completed...\n",
      "\n",
      "    Processing metafiles:\n",
      "\tStep completed...\n",
      "\n",
      "    Processing table information:\n",
      "\tStep completed...\n",
      "\n",
      "** Processing ScienceBase items **\n",
      "\n",
      "\tLogin to ScienceBase - ········\n",
      "\n",
      "   Processing dataset: erdCinpKfmSFNH into ScienceBase\n",
      "\n",
      "\tSB item: 5898f72ce4b0efcedb707861 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "   Processing dataset: erdCinpKfm1Q into ScienceBase\n",
      "\n",
      "\tSB item: 5898f73ce4b0efcedb707869 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "Info for dataset nwioosHudRef - incomplete not processed\n",
      "\n",
      "   Processing dataset: erdGlobecVpt into ScienceBase\n",
      "\n",
      "\tSB item: 5899018ae4b0efcedb7078be was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "   Processing dataset: erdGlobecMoc1 into ScienceBase\n",
      "\n",
      "\tSB item: 58990193e4b0efcedb7078c6 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "   Processing dataset: erdCinpKfmFT into ScienceBase\n",
      "\n",
      "\tSB item: 5899019de4b0efcedb7078ce was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "   Processing dataset: erdCinpKfm5Q into ScienceBase\n",
      "\n",
      "\tSB item: 589901a3e4b0efcedb7078d6 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "Info for dataset nwioosHudFishOccur - incomplete not processed\n",
      "\n",
      "   Processing dataset: erdGlobecBirds into ScienceBase\n",
      "\n",
      "\tSB item: 589901abe4b0efcedb7078df was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "   Processing dataset: erdCinpKfmRPC into ScienceBase\n",
      "\n",
      "\tSB item: 589901b3e4b0efcedb7078e8 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tuploading new file to item\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "Info for dataset nwioosHudFishDetails - incomplete not processed\n",
      "\n",
      "Info for dataset nwioosHudPredPrey - incomplete not processed\n",
      "\n",
      "Info for dataset prboSefiProd - incomplete not processed\n",
      "\n",
      "   Processing dataset: wocecpr into ScienceBase\n",
      "\n",
      "\tSB item: 589901b8e4b0efcedb7078f0 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "Info for dataset prboSefiPhen - incomplete not processed\n",
      "\n",
      "Info for dataset prboSefiDiet - incomplete not processed\n",
      "\n",
      "   Processing dataset: erdCinpKfmBT into ScienceBase\n",
      "\n",
      "\tSB item: 589901c1e4b0efcedb7078f9 was found in ScienceBase\n",
      "\t** Verified ** user has write privileges for item\n",
      "\tChecking files, uploading/updating files and metadata\n",
      "\tuploading new file to item\n",
      "\tuploading new file to item\n",
      "\tModifying ScienceBase record information\n",
      "\tSearching for duplicate sb items using filenames, item title, and dataset ID\n",
      "\t\tpass: item duplicates not identified\n",
      "\tScienceBase processing completed\n",
      "\n",
      "Info for dataset aadcArgos - incomplete not processed\n",
      "\n",
      "Info for dataset prboSefiPop - incomplete not processed\n",
      "\n",
      "Task 1: Preliminary processes and testing | Runtime [secs]: 0.497\n",
      "Task 2: Process datasets and information | Runtime [secs]: 25.617\n",
      "Task 3: Develop and/or modify ScienceBase records | Runtime [secs]: 114.881\n",
      "Task 4: Archive status and dictionary files with date stamps | Runtime [secs]: 0.006\n",
      "\n",
      "Program has finished"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/python\n",
    "\n",
    "# *******************************************************\n",
    "#                   Main Program: \n",
    "# *******************************************************\n",
    "\n",
    "def main(): \n",
    "    \n",
    "#  read general input file\n",
    "    with open(tempdir + \"name_file_dict.json\" ) as fp:    \n",
    "        nf_dict = json.load(fp)\n",
    "\n",
    "#  task timer\n",
    "    tstamp = []\n",
    "    tasklist = []\n",
    "    tasklist.append('Preliminary processes and testing')\n",
    "\n",
    "#  check preliminary info, search and retrieve ERDDAP data information\n",
    "    tstamp.append(timer())\n",
    "    sys.stdout.write('Search ERDDAP data repository, develop search criteria\\n') \n",
    "    SB_status = chk_files(tempdir, nf_dict, 'initialize')\n",
    "    df_request = retrieve_data(nf_dict, SB_status)\n",
    "\n",
    "#  reset data and metadata processing status in status file, if directed\n",
    "    purge_option(SB_status, nf_dict)\n",
    "    \n",
    "#  process data\n",
    "    tstamp.append(timer())\n",
    "    tasklist.append('Process datasets and information')\n",
    "    sys.stdout.write(\"{}\".format('\\n\\n** Processing data and metadata files\\n'))\n",
    "    data_proc(SB_status, df_request, nf_dict)\n",
    "\n",
    "#  create, modify sciencebase records \n",
    "    tstamp.append(timer())\n",
    "    tasklist.append('Develop and/or modify ScienceBase records')\n",
    "    sys.stdout.write(\"{}\".format('\\n** Processing ScienceBase items **'))\n",
    "    SciBase_item(SB_status, nf_dict)\n",
    "          \n",
    "#  create date_stamps \n",
    "    tstamp.append(timer())\n",
    "    tasklist.append('Archive status and dictionary files with date stamps')\n",
    "    dt = datetime.datetime.now()\n",
    "    timesec = int(dt.strftime('%s%f'))\n",
    "    today = datetime.datetime.strftime(dt,'%Y-%m-%d')\n",
    "    partial_uid = SB_status[0]['general information'][10]['processing uuid'].rsplit(\"-\",1)[1] \n",
    "    unique = 'uuid_' + partial_uid + '_timeid_' + str(timesec)\n",
    "    fulldate = datetime.datetime.strftime(dt,'%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "#  save final version of status file\n",
    "    SB_status[0]['general information'][2]['file modified (date-time)'] = fulldate\n",
    "    fname = \"{}{}\".format(SB_status[1]['local_directory'], nf_dict['init']['sb_json'])\n",
    "    with open(fname, 'w') as fp:\n",
    "        json.dump(SB_status, fp, indent=4)\n",
    "        \n",
    "#  create prov copies of status file and dictionary file, place in prov folder       \n",
    "    fpath = \"{}{}\".format(SB_status[1]['local_directory'], 'Prov_files')\n",
    "    chk_files(fpath, {}, '--')\n",
    "    fname = \"{}{}{}{}\".format(fpath, '/', unique, '_status.json')\n",
    "    with open(fname, 'w') as fp:\n",
    "        json.dump(SB_status, fp, indent=4)\n",
    "    fname = \"{}{}{}{}\".format(fpath, '/', unique, '_nfdict.json')\n",
    "    with open(fname, 'w') as fp: \n",
    "        json.dump(nf_dict, fp, indent=4)\n",
    "    \n",
    "#  timer results\n",
    "    tstamp.append(timer())\n",
    "    for i in range(len(tstamp)-1):\n",
    "        sys.stdout.write('\\nTask '+ str(i+1) + ': ' + tasklist[i] + ' | Runtime [secs]: '+ str(round(tstamp[i+1]-tstamp[i],3)))\n",
    "    \n",
    "    sys.stdout.write(\"\\n\\nProgram has finished\")\n",
    "\n",
    "    \n",
    "# ************************************************************************* \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "# call to main                                                 \n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
